#!/usr/bin/env python3
"""
æ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œæ•°æ®åˆ†æä»»åŠ¡ã€‚
åŒ…æ‹¬ï¼š
- æ•°æ®é¢„å¤„ç†
- ç»Ÿè®¡åˆ†æ
- å¯è§†åŒ–ç”Ÿæˆ
- æŠ¥å‘Šç”Ÿæˆ
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, Any, List
import random

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langgraph_multi_agent.system.integration import SystemIntegrator


def generate_sample_data() -> Dict[str, Any]:
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
    
    # æ¨¡æ‹Ÿé”€å”®æ•°æ®
    products = ["äº§å“A", "äº§å“B", "äº§å“C", "äº§å“D", "äº§å“E"]
    regions = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"]
    months = ["2024-01", "2024-02", "2024-03", "2024-04", "2024-05", "2024-06"]
    
    sales_data = []
    for month in months:
        for region in regions:
            for product in products:
                sales_data.append({
                    "month": month,
                    "region": region,
                    "product": product,
                    "sales": random.randint(1000, 10000),
                    "units": random.randint(10, 100),
                    "cost": random.randint(500, 5000)
                })
    
    return {
        "dataset_name": "é”€å”®æ•°æ®åˆ†æ",
        "description": "2024å¹´ä¸ŠåŠå¹´å„åœ°åŒºäº§å“é”€å”®æ•°æ®",
        "data": sales_data,
        "metadata": {
            "total_records": len(sales_data),
            "date_range": "2024-01 to 2024-06",
            "regions": regions,
            "products": products
        }
    }


async def data_analysis_workflow_example():
    """æ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹"""
    
    print("=" * 70)
    print("LangGraphå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ - æ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹")
    print("=" * 70)
    
    # 1. ç”Ÿæˆç¤ºä¾‹æ•°æ®
    print("\n1. ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
    
    sample_data = generate_sample_data()
    print(f"âœ… ç”Ÿæˆæ•°æ®é›†: {sample_data['dataset_name']}")
    print(f"   è®°å½•æ•°: {sample_data['metadata']['total_records']}")
    print(f"   æ—¶é—´èŒƒå›´: {sample_data['metadata']['date_range']}")
    
    # 2. åˆå§‹åŒ–ç³»ç»Ÿ
    print("\n2. åˆå§‹åŒ–åˆ†æç³»ç»Ÿ...")
    
    config = {
        "checkpoint_storage": "memory",
        "enable_metrics": True,
        "enable_tracing": True,
        "optimization_level": "moderate",
        "performance": {
            "max_cache_size": 5000,
            "enable_auto_optimization": True,
            "max_workers": 4
        }
    }
    
    integrator = SystemIntegrator(config)
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        success = await integrator.initialize_system()
        if not success:
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        print("âœ… åˆ†æç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # 3. åˆ›å»ºæ•°æ®åˆ†æå·¥ä½œæµ
        print("\n3. åˆ›å»ºæ•°æ®åˆ†æå·¥ä½œæµ...")
        
        workflow_id = f"data_analysis_{int(datetime.now().timestamp())}"
        workflow = await integrator.create_workflow(
            workflow_id=workflow_id,
            template_name="analysis",  # ä½¿ç”¨åˆ†ææ¨¡æ¿
            custom_config={
                "timeout_seconds": 600,  # 10åˆ†é’Ÿè¶…æ—¶
                "max_iterations": 100,
                "execution_mode": "sequential"
            }
        )
        
        print(f"âœ… æ•°æ®åˆ†æå·¥ä½œæµåˆ›å»ºæˆåŠŸ: {workflow_id}")
        
        # 4. å‡†å¤‡åˆ†æä»»åŠ¡
        print("\n4. å‡†å¤‡åˆ†æä»»åŠ¡...")
        
        analysis_task = {
            "task_id": f"analysis_task_{int(datetime.now().timestamp())}",
            "title": "é”€å”®æ•°æ®ç»¼åˆåˆ†æ",
            "description": "å¯¹2024å¹´ä¸ŠåŠå¹´é”€å”®æ•°æ®è¿›è¡Œå…¨é¢åˆ†æ",
            "task_type": "data_analysis",
            "data": sample_data,
            "analysis_requirements": [
                "æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—",
                "æè¿°æ€§ç»Ÿè®¡åˆ†æ",
                "é”€å”®è¶‹åŠ¿åˆ†æ",
                "åœ°åŒºé”€å”®å¯¹æ¯”",
                "äº§å“æ€§èƒ½åˆ†æ",
                "å¼‚å¸¸å€¼æ£€æµ‹",
                "é¢„æµ‹å»ºæ¨¡",
                "å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ",
                "åˆ†ææŠ¥å‘Šç”Ÿæˆ"
            ],
            "output_format": "comprehensive_report",
            "priority": 3,
            "constraints": {
                "max_processing_time": 600,
                "required_confidence": 0.85,
                "visualization_types": ["bar_chart", "line_chart", "heatmap", "scatter_plot"]
            }
        }
        
        print(f"âœ… åˆ†æä»»åŠ¡å‡†å¤‡å®Œæˆ")
        print(f"   ä»»åŠ¡ç±»å‹: {analysis_task['task_type']}")
        print(f"   åˆ†æè¦æ±‚: {len(analysis_task['analysis_requirements'])} é¡¹")
        
        # 5. æ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ
        print("\n5. æ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ...")
        print("â³ æ­£åœ¨è¿›è¡Œæ•°æ®åˆ†æï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        
        start_time = datetime.now()
        
        try:
            # åˆ†é˜¶æ®µæ˜¾ç¤ºè¿›åº¦
            print("   ğŸ“Š é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†å’Œè´¨é‡æ£€æŸ¥...")
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            print("   ğŸ“ˆ é˜¶æ®µ2: ç»Ÿè®¡åˆ†æå’Œè¶‹åŠ¿è¯†åˆ«...")
            await asyncio.sleep(1)
            
            print("   ğŸ¯ é˜¶æ®µ3: æ·±åº¦åˆ†æå’Œæ¨¡å¼å‘ç°...")
            await asyncio.sleep(1)
            
            print("   ğŸ“‹ é˜¶æ®µ4: æŠ¥å‘Šç”Ÿæˆå’Œç»“æœæ•´ç†...")
            
            result = await integrator.execute_workflow(
                workflow_id=workflow_id,
                task_input=analysis_task,
                config={
                    "recursion_limit": 150,
                    "timeout": 600
                }
            )
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            print(f"âœ… æ•°æ®åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {execution_time:.2f}ç§’")
            
            # 6. åˆ†æç»“æœå±•ç¤º
            print("\n6. åˆ†æç»“æœ:")
            print("=" * 50)
            
            if isinstance(result, dict):
                # æ˜¾ç¤ºåˆ†ææ‘˜è¦
                if "analysis_summary" in result:
                    summary = result["analysis_summary"]
                    print("ğŸ“Š åˆ†ææ‘˜è¦:")
                    for key, value in summary.items():
                        print(f"   {key}: {value}")
                
                # æ˜¾ç¤ºå…³é”®å‘ç°
                if "key_findings" in result:
                    findings = result["key_findings"]
                    print("\nğŸ” å…³é”®å‘ç°:")
                    for i, finding in enumerate(findings, 1):
                        print(f"   {i}. {finding}")
                
                # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
                if "statistics" in result:
                    stats = result["statistics"]
                    print("\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
                    for metric, value in stats.items():
                        print(f"   {metric}: {value}")
                
                # æ˜¾ç¤ºå»ºè®®
                if "recommendations" in result:
                    recommendations = result["recommendations"]
                    print("\nğŸ’¡ åˆ†æå»ºè®®:")
                    for i, rec in enumerate(recommendations, 1):
                        print(f"   {i}. {rec}")
            
            # 7. ç”Ÿæˆæ¨¡æ‹Ÿçš„è¯¦ç»†åˆ†æç»“æœ
            print("\n7. è¯¦ç»†åˆ†æç»“æœ:")
            print("-" * 50)
            
            # æ¨¡æ‹Ÿåˆ†æç»“æœ
            detailed_results = {
                "æ•°æ®è´¨é‡": {
                    "å®Œæ•´æ€§": "99.8%",
                    "å‡†ç¡®æ€§": "95.2%",
                    "å¼‚å¸¸å€¼": "2.1%",
                    "ç¼ºå¤±å€¼": "0.2%"
                },
                "é”€å”®è¶‹åŠ¿": {
                    "æ€»ä½“è¶‹åŠ¿": "ä¸Šå‡",
                    "å¢é•¿ç‡": "15.3%",
                    "å­£èŠ‚æ€§": "æ˜æ˜¾",
                    "æ³¢åŠ¨æ€§": "ä¸­ç­‰"
                },
                "åœ°åŒºåˆ†æ": {
                    "æœ€ä½³åœ°åŒº": "ä¸Šæµ·",
                    "å¢é•¿æœ€å¿«": "æ·±åœ³",
                    "æ½œåŠ›åœ°åŒº": "æ­å·",
                    "éœ€å…³æ³¨": "åŒ—äº¬"
                },
                "äº§å“åˆ†æ": {
                    "çƒ­é”€äº§å“": "äº§å“C",
                    "åˆ©æ¶¦æœ€é«˜": "äº§å“A",
                    "å¢é•¿æœ€å¿«": "äº§å“E",
                    "éœ€ä¼˜åŒ–": "äº§å“D"
                }
            }
            
            for category, metrics in detailed_results.items():
                print(f"\n{category}:")
                for metric, value in metrics.items():
                    print(f"  â€¢ {metric}: {value}")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"åˆ†æå·¥ä½œæµé”™è¯¯: {e}")
        
        # 8. æ€§èƒ½åˆ†æ
        print("\n8. æ€§èƒ½åˆ†æ:")
        print("-" * 50)
        
        # è·å–ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
        status = await integrator.get_system_status()
        
        if "system_performance" in status:
            perf = status["system_performance"]
            
            print("ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡:")
            
            # ç¼“å­˜æ€§èƒ½
            if "cache" in perf:
                cache_stats = perf["cache"]["global_stats"]
                print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.2%}")
                print(f"  ç¼“å­˜ä½¿ç”¨é‡: {cache_stats.get('total_size', 0)} é¡¹")
            
            # å¹¶å‘æ‰§è¡Œæ€§èƒ½
            if "concurrent_executor" in perf:
                exec_stats = perf["concurrent_executor"]
                print(f"  ç³»ç»Ÿè´Ÿè½½: {exec_stats.get('current_load', 0):.2%}")
                print(f"  ä»»åŠ¡ç»Ÿè®¡: æäº¤ {exec_stats.get('stats', {}).get('submitted', 0)}, "
                      f"å®Œæˆ {exec_stats.get('stats', {}).get('completed', 0)}")
        
        # 9. ä¼˜åŒ–å»ºè®®
        print("\n9. ç³»ç»Ÿä¼˜åŒ–å»ºè®®:")
        print("-" * 50)
        
        recommendations = integrator.get_performance_recommendations()
        
        if isinstance(recommendations, dict) and "error" not in recommendations:
            for category, suggestions in recommendations.items():
                if suggestions and isinstance(suggestions, list):
                    print(f"{category}:")
                    for suggestion in suggestions:
                        print(f"  ğŸ’¡ {suggestion}")
        
        # 10. æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–
        print("\n10. æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–...")
        
        optimization_result = await integrator.optimize_system_performance()
        
        if "error" not in optimization_result:
            print(f"âœ… ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")
            print(f"   åº”ç”¨ä¼˜åŒ–ç­–ç•¥: {optimization_result.get('optimization_results', 0)} ä¸ª")
            print(f"   æ€§èƒ½æå‡: {optimization_result.get('total_improvement', 0):.1f}%")
        else:
            print(f"âš ï¸  ç³»ç»Ÿä¼˜åŒ–é‡åˆ°é—®é¢˜: {optimization_result['error']}")
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹æ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"æ•°æ®åˆ†æç¤ºä¾‹é”™è¯¯: {e}")
    
    finally:
        # 11. æ¸…ç†èµ„æº
        print("\n11. æ¸…ç†èµ„æº...")
        
        try:
            await integrator.shutdown_system()
            print("âœ… ç³»ç»Ÿå…³é—­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  ç³»ç»Ÿå…³é—­æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    print("\n" + "=" * 70)
    print("æ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹å®Œæˆ")
    print("=" * 70)


def main():
    """ä¸»å‡½æ•°"""
    try:
        asyncio.run(data_analysis_workflow_example())
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {e}")


if __name__ == "__main__":
    main()