"""
æ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•å¥—ä»¶
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional

from ..core.test_controller import TestSuite
from ..core.test_result import TestResult, TestSuiteResult, TestStatus, ResultType
from ..config.test_config import TestConfiguration
from ..utils.helpers import retry_on_failure, timeout_after
from ..utils.logging_utils import get_logger
from ..utils.data_utils import TestDataGenerator


class AgentTestSuite(TestSuite):
    """æ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, name: str, config: TestConfiguration):
        super().__init__(name, config)
        self.logger = get_logger("agent_test")
        self.data_generator = TestDataGenerator()
    
    async def run_all_tests(self) -> TestSuiteResult:
        """è¿è¡Œæ‰€æœ‰æ™ºèƒ½ä½“æµ‹è¯•"""
        self.logger.info("ğŸ¤– å¼€å§‹æ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•å¥—ä»¶")
        
        suite_result = TestSuiteResult(
            suite_name=self.name,
            start_time=datetime.now()
        )
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            ("meta_agent", self._test_meta_agent),
            ("task_decomposer", self._test_task_decomposer),
            ("coordinator", self._test_coordinator),
            ("llm_integration", self._test_llm_integration)
        ]
        
        # æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
        for test_name, test_func in test_cases:
            result = await self._run_single_test(test_name, test_func)
            suite_result.add_result(result)
        
        suite_result.end_time = datetime.now()
        suite_result.calculate_duration()
        
        self.logger.info(f"ğŸ¤– æ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•å¥—ä»¶å®Œæˆ: {suite_result.passed_tests}/{suite_result.total_tests} é€šè¿‡")
        
        return suite_result
    
    async def _run_single_test(self, test_name: str, test_func) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        result = self.create_test_result(test_name)
        result.start_time = datetime.now()
        result.result_type = ResultType.FUNCTIONAL
        result.tags = ["agent", "ai"]
        
        self.logger.test_start(test_name)
        
        try:
            success, details = await test_func()
            
            result.end_time = datetime.now()
            result.calculate_duration()
            
            if success:
                result.set_success(details)
                self.logger.test_end(test_name, True, result.duration)
            else:
                error_msg = details.get("error", "Unknown error") if isinstance(details, dict) else str(details)
                result.set_error(error_msg)
                self.logger.test_end(test_name, False, result.duration)
            
        except Exception as e:
            result.end_time = datetime.now()
            result.calculate_duration()
            result.set_error(str(e))
            self.logger.error_detail(e, {"test_name": test_name})
        
        return result
    
    @timeout_after(180.0)
    async def _test_meta_agent(self) -> Tuple[bool, Dict[str, Any]]:
        """æµ‹è¯•MetaAgentåŠŸèƒ½"""
        self.logger.test_step("æµ‹è¯•MetaAgentä»»åŠ¡åˆ†æåŠŸèƒ½")
        
        # æ¨¡æ‹ŸMetaAgentæµ‹è¯•ï¼ˆä¸å®é™…è°ƒç”¨LLMï¼‰
        test_tasks = [
            self.data_generator.generate_task_data("analysis", "simple"),
            self.data_generator.generate_task_data("processing", "medium"),
            self.data_generator.generate_task_data("complex_workflow", "complex")
        ]
        
        results = {
            "tasks_analyzed": 0,
            "analysis_results": [],
            "average_analysis_time": 0,
            "complexity_scores": []
        }
        
        total_analysis_time = 0
        
        for i, task in enumerate(test_tasks):
            try:
                start_time = time.time()
                
                # æ¨¡æ‹ŸMetaAgentåˆ†æ
                mock_analysis = self._mock_meta_agent_analysis(task)
                
                analysis_time = time.time() - start_time
                total_analysis_time += analysis_time
                
                results["tasks_analyzed"] += 1
                results["analysis_results"].append({
                    "task_id": task["task_id"],
                    "complexity_score": mock_analysis["complexity_score"],
                    "requires_decomposition": mock_analysis["requires_decomposition"],
                    "estimated_time": mock_analysis["estimated_time"],
                    "analysis_time": analysis_time
                })
                results["complexity_scores"].append(mock_analysis["complexity_score"])
                
                self.logger.test_step(f"âœ… ä»»åŠ¡ {i+1} åˆ†æå®Œæˆ - å¤æ‚åº¦: {mock_analysis['complexity_score']:.2f}")
                
            except Exception as e:
                self.logger.test_step(f"âŒ ä»»åŠ¡ {i+1} åˆ†æå¤±è´¥: {e}")
        
        if results["tasks_analyzed"] > 0:
            results["average_analysis_time"] = total_analysis_time / results["tasks_analyzed"]
            results["average_complexity"] = sum(results["complexity_scores"]) / len(results["complexity_scores"])
        
        success = results["tasks_analyzed"] == len(test_tasks)
        
        return success, results
    
    @timeout_after(180.0)
    async def _test_task_decomposer(self) -> Tuple[bool, Dict[str, Any]]:
        """æµ‹è¯•TaskDecomposeråŠŸèƒ½"""
        self.logger.test_step("æµ‹è¯•TaskDecomposerä»»åŠ¡åˆ†è§£åŠŸèƒ½")
        
        # åˆ›å»ºéœ€è¦åˆ†è§£çš„å¤æ‚ä»»åŠ¡
        complex_task = self.data_generator.generate_task_data("complex_analysis", "complex")
        
        results = {
            "decomposition_successful": False,
            "subtasks_generated": 0,
            "decomposition_time": 0,
            "subtask_details": []
        }
        
        try:
            start_time = time.time()
            
            # æ¨¡æ‹Ÿä»»åŠ¡åˆ†è§£
            mock_decomposition = self._mock_task_decomposition(complex_task)
            
            decomposition_time = time.time() - start_time
            
            results["decomposition_successful"] = True
            results["subtasks_generated"] = len(mock_decomposition["subtasks"])
            results["decomposition_time"] = decomposition_time
            results["subtask_details"] = mock_decomposition["subtasks"]
            results["dependencies"] = mock_decomposition.get("dependencies", [])
            
            self.logger.test_step(f"âœ… ä»»åŠ¡åˆ†è§£æˆåŠŸ - ç”Ÿæˆ {len(mock_decomposition['subtasks'])} ä¸ªå­ä»»åŠ¡")
            
        except Exception as e:
            results["error"] = str(e)
            self.logger.test_step(f"âŒ ä»»åŠ¡åˆ†è§£å¤±è´¥: {e}")
        
        success = results["decomposition_successful"] and results["subtasks_generated"] > 0
        
        return success, results
    
    @timeout_after(120.0)
    async def _test_coordinator(self) -> Tuple[bool, Dict[str, Any]]:
        """æµ‹è¯•CoordinatoråŠŸèƒ½"""
        self.logger.test_step("æµ‹è¯•Coordinatoræ™ºèƒ½ä½“åè°ƒåŠŸèƒ½")
        
        # æ¨¡æ‹Ÿå¤šä¸ªæ™ºèƒ½ä½“ä»»åŠ¡
        agent_tasks = [
            {"agent_id": "agent_1", "task_type": "analysis", "priority": 1},
            {"agent_id": "agent_2", "task_type": "processing", "priority": 2},
            {"agent_id": "agent_3", "task_type": "validation", "priority": 3}
        ]
        
        results = {
            "coordination_successful": False,
            "agents_coordinated": 0,
            "coordination_time": 0,
            "execution_plan": [],
            "resource_allocation": {}
        }
        
        try:
            start_time = time.time()
            
            # æ¨¡æ‹Ÿåè°ƒè¿‡ç¨‹
            mock_coordination = self._mock_coordinator_planning(agent_tasks)
            
            coordination_time = time.time() - start_time
            
            results["coordination_successful"] = True
            results["agents_coordinated"] = len(agent_tasks)
            results["coordination_time"] = coordination_time
            results["execution_plan"] = mock_coordination["execution_plan"]
            results["resource_allocation"] = mock_coordination["resource_allocation"]
            
            self.logger.test_step(f"âœ… æ™ºèƒ½ä½“åè°ƒæˆåŠŸ - åè°ƒ {len(agent_tasks)} ä¸ªæ™ºèƒ½ä½“")
            
        except Exception as e:
            results["error"] = str(e)
            self.logger.test_step(f"âŒ æ™ºèƒ½ä½“åè°ƒå¤±è´¥: {e}")
        
        success = results["coordination_successful"] and results["agents_coordinated"] > 0
        
        return success, results
    
    @timeout_after(60.0)
    async def _test_llm_integration(self) -> Tuple[bool, Dict[str, Any]]:
        """æµ‹è¯•LLMé›†æˆ"""
        self.logger.test_step("æµ‹è¯•LLMé›†æˆåŠŸèƒ½")
        
        results = {
            "llm_client_available": False,
            "configuration_valid": False,
            "mock_request_successful": False,
            "response_time": 0
        }
        
        try:
            # æ£€æŸ¥LLMå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            try:
                # å°è¯•å¯¼å…¥LLMå®¢æˆ·ç«¯
                import sys
                import os
                
                # æ·»åŠ é¡¹ç›®è·¯å¾„
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
                if project_root not in sys.path:
                    sys.path.append(project_root)
                
                from langgraph_multi_agent.llm.siliconflow_client import SiliconFlowClient
                
                client = SiliconFlowClient()
                results["llm_client_available"] = True
                
                # æ£€æŸ¥é…ç½®
                if hasattr(client, 'api_key') and client.api_key:
                    results["configuration_valid"] = True
                    self.logger.test_step("âœ… LLMå®¢æˆ·ç«¯é…ç½®æœ‰æ•ˆ")
                else:
                    self.logger.test_step("âš ï¸ LLMå®¢æˆ·ç«¯é…ç½®æ— æ•ˆæˆ–ç¼ºå¤±")
                
            except ImportError as e:
                self.logger.test_step(f"âš ï¸ LLMå®¢æˆ·ç«¯ä¸å¯ç”¨: {e}")
            except Exception as e:
                self.logger.test_step(f"âš ï¸ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # æ¨¡æ‹ŸLLMè¯·æ±‚ï¼ˆä¸å®é™…è°ƒç”¨APIï¼‰
            start_time = time.time()
            
            mock_response = self._mock_llm_request("æµ‹è¯•æç¤ºè¯")
            
            response_time = time.time() - start_time
            
            results["mock_request_successful"] = True
            results["response_time"] = response_time
            results["mock_response"] = mock_response
            
            self.logger.test_step("âœ… LLMæ¨¡æ‹Ÿè¯·æ±‚æˆåŠŸ")
            
        except Exception as e:
            results["error"] = str(e)
            self.logger.test_step(f"âŒ LLMé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        
        # å¦‚æœå®¢æˆ·ç«¯å¯ç”¨æˆ–æ¨¡æ‹Ÿè¯·æ±‚æˆåŠŸï¼Œåˆ™è®¤ä¸ºæµ‹è¯•é€šè¿‡
        success = results["llm_client_available"] or results["mock_request_successful"]
        
        return success, results
    
    def _mock_meta_agent_analysis(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸMetaAgentåˆ†æ"""
        # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æç»“æœ
        task_type = task.get("task_type", "general")
        description = task.get("description", "")
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        complexity_score = 0.3  # åŸºç¡€å¤æ‚åº¦
        
        if "complex" in task_type.lower() or "complex" in description.lower():
            complexity_score += 0.4
        if len(task.get("requirements", [])) > 3:
            complexity_score += 0.2
        if len(task.get("constraints", [])) > 2:
            complexity_score += 0.1
        
        complexity_score = min(complexity_score, 1.0)
        
        return {
            "complexity_score": complexity_score,
            "requires_decomposition": complexity_score > 0.6,
            "estimated_time": int(complexity_score * 1800),  # æœ€å¤š30åˆ†é’Ÿ
            "recommended_agents": ["meta_agent", "coordinator"] if complexity_score > 0.5 else ["meta_agent"],
            "confidence": 0.85
        }
    
    def _mock_task_decomposition(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿä»»åŠ¡åˆ†è§£"""
        subtasks = []
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆå­ä»»åŠ¡
        task_type = task.get("task_type", "general")
        
        if "analysis" in task_type.lower():
            subtasks = [
                {"name": "æ•°æ®æ”¶é›†", "type": "data_collection", "priority": 1},
                {"name": "æ•°æ®é¢„å¤„ç†", "type": "preprocessing", "priority": 2},
                {"name": "åˆ†ææ‰§è¡Œ", "type": "analysis", "priority": 3},
                {"name": "ç»“æœéªŒè¯", "type": "validation", "priority": 4}
            ]
        elif "processing" in task_type.lower():
            subtasks = [
                {"name": "è¾“å…¥éªŒè¯", "type": "validation", "priority": 1},
                {"name": "æ•°æ®å¤„ç†", "type": "processing", "priority": 2},
                {"name": "è¾“å‡ºç”Ÿæˆ", "type": "output", "priority": 3}
            ]
        else:
            subtasks = [
                {"name": "ä»»åŠ¡å‡†å¤‡", "type": "preparation", "priority": 1},
                {"name": "ä»»åŠ¡æ‰§è¡Œ", "type": "execution", "priority": 2},
                {"name": "ç»“æœæ•´ç†", "type": "finalization", "priority": 3}
            ]
        
        return {
            "subtasks": subtasks,
            "dependencies": [
                {"from": subtasks[i]["name"], "to": subtasks[i+1]["name"]} 
                for i in range(len(subtasks)-1)
            ],
            "estimated_total_time": len(subtasks) * 300  # æ¯ä¸ªå­ä»»åŠ¡5åˆ†é’Ÿ
        }
    
    def _mock_coordinator_planning(self, agent_tasks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿåè°ƒå™¨è§„åˆ’"""
        # æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
        sorted_tasks = sorted(agent_tasks, key=lambda x: x.get("priority", 999))
        
        execution_plan = []
        resource_allocation = {}
        
        for i, task in enumerate(sorted_tasks):
            execution_plan.append({
                "step": i + 1,
                "agent_id": task["agent_id"],
                "task_type": task["task_type"],
                "estimated_start_time": i * 300,  # æ¯ä¸ªä»»åŠ¡é—´éš”5åˆ†é’Ÿ
                "estimated_duration": 600  # æ¯ä¸ªä»»åŠ¡10åˆ†é’Ÿ
            })
            
            resource_allocation[task["agent_id"]] = {
                "cpu_allocation": 0.5,
                "memory_allocation": "512MB",
                "priority_level": task.get("priority", 5)
            }
        
        return {
            "execution_plan": execution_plan,
            "resource_allocation": resource_allocation,
            "total_estimated_time": len(sorted_tasks) * 600,
            "parallel_execution_possible": len(sorted_tasks) <= 3
        }
    
    def _mock_llm_request(self, prompt: str) -> Dict[str, Any]:
        """æ¨¡æ‹ŸLLMè¯·æ±‚"""
        return {
            "response": f"è¿™æ˜¯å¯¹æç¤ºè¯ '{prompt[:50]}...' çš„æ¨¡æ‹Ÿå“åº”",
            "tokens_used": len(prompt.split()) + 20,
            "model": "mock_model",
            "success": True,
            "latency": 0.5
        }