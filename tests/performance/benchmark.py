#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿™ä¸ªè„šæœ¬å¯¹LangGraphå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚
åŒ…æ‹¬ï¼š
- å•ä»»åŠ¡å¤„ç†åŸºå‡†
- æ‰¹é‡ä»»åŠ¡å¤„ç†åŸºå‡†
- ä¸åŒå·¥ä½œæµæ¨¡æ¿æ€§èƒ½å¯¹æ¯”
- ç¼“å­˜æ€§èƒ½æµ‹è¯•
- èµ„æºä½¿ç”¨æ•ˆç‡æµ‹è¯•
"""

import asyncio
import logging
import time
import psutil
import statistics
from datetime import datetime
from typing import Dict, Any, List
import json
import matplotlib.pyplot as plt
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from langgraph_multi_agent.system.integration import SystemIntegrator


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, integrator: SystemIntegrator):
        self.integrator = integrator
        self.results = {}
        
    async def run_single_task_benchmark(self, iterations: int = 50) -> Dict[str, Any]:
        """å•ä»»åŠ¡å¤„ç†åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š å•ä»»åŠ¡å¤„ç†åŸºå‡†æµ‹è¯• - {iterations} æ¬¡è¿­ä»£")
        
        response_times = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(iterations):
            # ç›‘æ§ç³»ç»Ÿèµ„æº
            cpu_before = psutil.cpu_percent()
            memory_before = psutil.virtual_memory().percent
            
            # æ‰§è¡Œå•ä¸ªä»»åŠ¡
            start_time = time.time()
            
            try:
                task_data = {
                    "task_id": f"benchmark_single_{i}",
                    "title": f"åŸºå‡†æµ‹è¯•ä»»åŠ¡ {i}",
                    "description": "å•ä»»åŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•",
                    "requirements": ["å¤„ç†è¾“å…¥", "ç”Ÿæˆè¾“å‡º"]
                }
                
                workflow_id = f"benchmark_workflow_{i}"
                await self.integrator.create_workflow(workflow_id, "simple")
                result = await self.integrator.execute_workflow(workflow_id, task_data)
                
                response_time = time.time() - start_time
                response_times.append(response_time)
                
                # ç›‘æ§èµ„æºä½¿ç”¨
                cpu_after = psutil.cpu_percent()
                memory_after = psutil.virtual_memory().percent
                
                cpu_usage.append(max(cpu_after - cpu_before, 0))
                memory_usage.append(max(memory_after - memory_before, 0))
                
            except Exception as e:
                logger.error(f"åŸºå‡†æµ‹è¯•ä»»åŠ¡ {i} å¤±è´¥: {e}")
                response_times.append(float('inf'))
            
            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                print(f"   è¿›åº¦: {i + 1}/{iterations}")
        
        # è¿‡æ»¤æ— æ•ˆç»“æœ
        valid_times = [t for t in response_times if t != float('inf')]
        
        if not valid_times:
            return {"error": "æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†"}
        
        result = {
            "iterations": iterations,
            "successful": len(valid_times),
            "failed": iterations - len(valid_times),
            "response_time": {
                "min": min(valid_times),
                "max": max(valid_times),
                "avg": statistics.mean(valid_times),
                "median": statistics.median(valid_times),
                "std": statistics.stdev(valid_times) if len(valid_times) > 1 else 0,
                "p95": statistics.quantiles(valid_times, n=20)[18] if len(valid_times) >= 20 else max(valid_times),
                "p99": statistics.quantiles(valid_times, n=100)[98] if len(valid_times) >= 100 else max(valid_times)
            },
            "resource_usage": {
                "avg_cpu_delta": statistics.mean(cpu_usage) if cpu_usage else 0,
                "avg_memory_delta": statistics.mean(memory_usage) if memory_usage else 0
            }
        }
        
        print(f"âœ… å•ä»»åŠ¡åŸºå‡†æµ‹è¯•å®Œæˆ")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {result['response_time']['avg']:.3f} ç§’")
        print(f"   P95å“åº”æ—¶é—´: {result['response_time']['p95']:.3f} ç§’")
        print(f"   æˆåŠŸç‡: {result['successful']/iterations:.2%}")
        
        return result
    
    async def run_batch_processing_benchmark(self, batch_sizes: List[int] = [1, 5, 10, 20, 50]) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“¦ æ‰¹é‡å¤„ç†åŸºå‡†æµ‹è¯• - æ‰¹æ¬¡å¤§å°: {batch_sizes}")
        
        batch_results = {}
        
        for batch_size in batch_sizes:
            print(f"   æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            start_time = time.time()
            
            # åˆ›å»ºæ‰¹é‡ä»»åŠ¡
            tasks = []
            for i in range(batch_size):
                task_data = {
                    "task_id": f"batch_{batch_size}_{i}",
                    "title": f"æ‰¹é‡ä»»åŠ¡ {i}",
                    "description": f"æ‰¹æ¬¡å¤§å° {batch_size} çš„ä»»åŠ¡ {i}",
                    "requirements": ["æ‰¹é‡å¤„ç†", "å¹¶å‘æ‰§è¡Œ"]
                }
                tasks.append(task_data)
            
            # å¹¶å‘æ‰§è¡Œæ‰¹é‡ä»»åŠ¡
            successful = 0
            failed = 0
            
            async def execute_task(task_data):
                try:
                    workflow_id = f"batch_workflow_{task_data['task_id']}"
                    await self.integrator.create_workflow(workflow_id, "simple")
                    result = await self.integrator.execute_workflow(workflow_id, task_data)
                    return True
                except Exception as e:
                    logger.error(f"æ‰¹é‡ä»»åŠ¡å¤±è´¥: {e}")
                    return False
            
            results = await asyncio.gather(*[execute_task(task) for task in tasks], return_exceptions=True)
            
            successful = sum(1 for r in results if r is True)
            failed = len(results) - successful
            
            total_time = time.time() - start_time
            throughput = batch_size / total_time if total_time > 0 else 0
            
            batch_results[batch_size] = {
                "batch_size": batch_size,
                "total_time": total_time,
                "successful": successful,
                "failed": failed,
                "throughput": throughput,
                "avg_time_per_task": total_time / batch_size if batch_size > 0 else 0
            }
            
            print(f"     å®Œæˆæ—¶é—´: {total_time:.2f} ç§’")
            print(f"     ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’")
            print(f"     æˆåŠŸç‡: {successful/batch_size:.2%}")
        
        print(f"âœ… æ‰¹é‡å¤„ç†åŸºå‡†æµ‹è¯•å®Œæˆ")
        
        return batch_results
    
    async def run_workflow_template_comparison(self) -> Dict[str, Any]:
        """å·¥ä½œæµæ¨¡æ¿æ€§èƒ½å¯¹æ¯”"""
        print(f"\nğŸ”„ å·¥ä½œæµæ¨¡æ¿æ€§èƒ½å¯¹æ¯”")
        
        templates = ["simple", "analysis", "complex", "parallel"]
        template_results = {}
        
        for template in templates:
            print(f"   æµ‹è¯•æ¨¡æ¿: {template}")
            
            response_times = []
            
            # æ¯ä¸ªæ¨¡æ¿æµ‹è¯•10æ¬¡
            for i in range(10):
                try:
                    task_data = {
                        "task_id": f"template_test_{template}_{i}",
                        "title": f"æ¨¡æ¿æµ‹è¯• {template} {i}",
                        "description": f"æµ‹è¯• {template} æ¨¡æ¿æ€§èƒ½",
                        "requirements": ["æ¨¡æ¿å¤„ç†", "æ€§èƒ½æµ‹è¯•"]
                    }
                    
                    start_time = time.time()
                    
                    workflow_id = f"template_workflow_{template}_{i}"
                    await self.integrator.create_workflow(workflow_id, template)
                    result = await self.integrator.execute_workflow(workflow_id, task_data)
                    
                    response_time = time.time() - start_time
                    response_times.append(response_time)
                    
                except Exception as e:
                    logger.error(f"æ¨¡æ¿ {template} æµ‹è¯• {i} å¤±è´¥: {e}")
                    response_times.append(float('inf'))
            
            # è¿‡æ»¤æ— æ•ˆç»“æœ
            valid_times = [t for t in response_times if t != float('inf')]
            
            if valid_times:
                template_results[template] = {
                    "template": template,
                    "iterations": 10,
                    "successful": len(valid_times),
                    "avg_response_time": statistics.mean(valid_times),
                    "min_response_time": min(valid_times),
                    "max_response_time": max(valid_times),
                    "success_rate": len(valid_times) / 10
                }
                
                print(f"     å¹³å‡å“åº”æ—¶é—´: {template_results[template]['avg_response_time']:.3f} ç§’")
                print(f"     æˆåŠŸç‡: {template_results[template]['success_rate']:.2%}")
            else:
                template_results[template] = {
                    "template": template,
                    "error": "æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†"
                }
        
        print(f"âœ… å·¥ä½œæµæ¨¡æ¿å¯¹æ¯”å®Œæˆ")
        
        return template_results
    
    async def run_cache_performance_test(self, cache_sizes: List[int] = [100, 500, 1000, 5000]) -> Dict[str, Any]:
        """ç¼“å­˜æ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸ’¾ ç¼“å­˜æ€§èƒ½æµ‹è¯• - ç¼“å­˜å¤§å°: {cache_sizes}")
        
        cache_results = {}
        
        # è·å–ç¼“å­˜ç®¡ç†å™¨
        cache_manager = self.integrator.get_cache_manager()
        if not cache_manager:
            return {"error": "ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨"}
        
        for cache_size in cache_sizes:
            print(f"   æµ‹è¯•ç¼“å­˜å¤§å°: {cache_size}")
            
            # æ¸…ç©ºç¼“å­˜
            cache_manager.clear_cache()
            
            # é¢„çƒ­ç¼“å­˜
            cache_data = {}
            for i in range(cache_size):
                key = f"cache_key_{i}"
                value = f"cache_value_{i}" * 10  # å¢åŠ æ•°æ®å¤§å°
                cache_data[key] = value
            
            # æµ‹è¯•ç¼“å­˜å†™å…¥æ€§èƒ½
            write_start = time.time()
            for key, value in cache_data.items():
                cache_manager.set(key, value)
            write_time = time.time() - write_start
            
            # æµ‹è¯•ç¼“å­˜è¯»å–æ€§èƒ½
            read_start = time.time()
            hit_count = 0
            for key in cache_data.keys():
                if cache_manager.get(key) is not None:
                    hit_count += 1
            read_time = time.time() - read_start
            
            hit_rate = hit_count / cache_size if cache_size > 0 else 0
            
            cache_results[cache_size] = {
                "cache_size": cache_size,
                "write_time": write_time,
                "read_time": read_time,
                "write_ops_per_sec": cache_size / write_time if write_time > 0 else 0,
                "read_ops_per_sec": cache_size / read_time if read_time > 0 else 0,
                "hit_rate": hit_rate
            }
            
            print(f"     å†™å…¥é€Ÿåº¦: {cache_results[cache_size]['write_ops_per_sec']:.0f} ops/sec")
            print(f"     è¯»å–é€Ÿåº¦: {cache_results[cache_size]['read_ops_per_sec']:.0f} ops/sec")
            print(f"     å‘½ä¸­ç‡: {hit_rate:.2%}")
        
        print(f"âœ… ç¼“å­˜æ€§èƒ½æµ‹è¯•å®Œæˆ")
        
        return cache_results
    
    async def run_resource_efficiency_test(self, duration_seconds: int = 60) -> Dict[str, Any]:
        """èµ„æºä½¿ç”¨æ•ˆç‡æµ‹è¯•"""
        print(f"\nâš¡ èµ„æºä½¿ç”¨æ•ˆç‡æµ‹è¯• - æŒç»­ {duration_seconds} ç§’")
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        task_count = 0
        cpu_samples = []
        memory_samples = []
        
        while time.time() < end_time:
            # æ‰§è¡Œä»»åŠ¡
            try:
                task_data = {
                    "task_id": f"efficiency_test_{task_count}",
                    "title": f"æ•ˆç‡æµ‹è¯•ä»»åŠ¡ {task_count}",
                    "description": "èµ„æºæ•ˆç‡æµ‹è¯•ä»»åŠ¡",
                    "requirements": ["èµ„æºç›‘æ§", "æ•ˆç‡æµ‹è¯•"]
                }
                
                workflow_id = f"efficiency_workflow_{task_count}"
                await self.integrator.create_workflow(workflow_id, "simple")
                result = await self.integrator.execute_workflow(workflow_id, task_data)
                
                task_count += 1
                
            except Exception as e:
                logger.error(f"æ•ˆç‡æµ‹è¯•ä»»åŠ¡å¤±è´¥: {e}")
            
            # é‡‡æ ·ç³»ç»Ÿèµ„æº
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            
            cpu_samples.append(cpu_percent)
            memory_samples.append(memory_percent)
            
            # çŸ­æš‚ä¼‘æ¯
            await asyncio.sleep(0.1)
        
        total_time = time.time() - start_time
        
        result = {
            "test_duration": total_time,
            "tasks_completed": task_count,
            "tasks_per_second": task_count / total_time if total_time > 0 else 0,
            "resource_usage": {
                "avg_cpu": statistics.mean(cpu_samples) if cpu_samples else 0,
                "max_cpu": max(cpu_samples) if cpu_samples else 0,
                "avg_memory": statistics.mean(memory_samples) if memory_samples else 0,
                "max_memory": max(memory_samples) if memory_samples else 0
            },
            "efficiency_score": task_count / (statistics.mean(cpu_samples) + statistics.mean(memory_samples)) if cpu_samples and memory_samples else 0
        }
        
        print(f"âœ… èµ„æºæ•ˆç‡æµ‹è¯•å®Œæˆ")
        print(f"   å®Œæˆä»»åŠ¡: {task_count} ä¸ª")
        print(f"   ä»»åŠ¡é€Ÿç‡: {result['tasks_per_second']:.2f} ä»»åŠ¡/ç§’")
        print(f"   å¹³å‡CPUä½¿ç”¨: {result['resource_usage']['avg_cpu']:.1f}%")
        print(f"   å¹³å‡å†…å­˜ä½¿ç”¨: {result['resource_usage']['avg_memory']:.1f}%")
        print(f"   æ•ˆç‡åˆ†æ•°: {result['efficiency_score']:.2f}")
        
        return result


async def run_comprehensive_benchmark():
    """è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•"""
    
    print("=" * 80)
    print("LangGraphå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ - æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    print("\nğŸš€ åˆå§‹åŒ–åŸºå‡†æµ‹è¯•ç³»ç»Ÿ...")
    
    config = {
        "checkpoint_storage": "memory",
        "enable_metrics": True,
        "enable_tracing": False,
        "optimization_level": "moderate",
        "performance": {
            "max_cache_size": 10000,
            "enable_auto_optimization": False,  # å…³é—­è‡ªåŠ¨ä¼˜åŒ–ä»¥è·å¾—ä¸€è‡´çš„åŸºå‡†
            "max_workers": 4,
            "execution_mode": "adaptive"
        }
    }
    
    integrator = SystemIntegrator(config)
    
    try:
        success = await integrator.initialize_system()
        if not success:
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        print("âœ… åŸºå‡†æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºåŸºå‡†æµ‹è¯•è¿è¡Œå™¨
        benchmark_runner = BenchmarkRunner(integrator)
        
        # æ”¶é›†æ‰€æœ‰åŸºå‡†æµ‹è¯•ç»“æœ
        all_results = {}
        
        # 1. å•ä»»åŠ¡å¤„ç†åŸºå‡†
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯• 1/5: å•ä»»åŠ¡å¤„ç†æ€§èƒ½")
        print("="*60)
        
        single_task_result = await benchmark_runner.run_single_task_benchmark(iterations=100)
        all_results["single_task"] = single_task_result
        
        # 2. æ‰¹é‡å¤„ç†åŸºå‡†
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯• 2/5: æ‰¹é‡å¤„ç†æ€§èƒ½")
        print("="*60)
        
        batch_result = await benchmark_runner.run_batch_processing_benchmark()
        all_results["batch_processing"] = batch_result
        
        # 3. å·¥ä½œæµæ¨¡æ¿å¯¹æ¯”
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯• 3/5: å·¥ä½œæµæ¨¡æ¿æ€§èƒ½å¯¹æ¯”")
        print("="*60)
        
        template_result = await benchmark_runner.run_workflow_template_comparison()
        all_results["workflow_templates"] = template_result
        
        # 4. ç¼“å­˜æ€§èƒ½æµ‹è¯•
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯• 4/5: ç¼“å­˜æ€§èƒ½")
        print("="*60)
        
        cache_result = await benchmark_runner.run_cache_performance_test()
        all_results["cache_performance"] = cache_result
        
        # 5. èµ„æºæ•ˆç‡æµ‹è¯•
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯• 5/5: èµ„æºä½¿ç”¨æ•ˆç‡")
        print("="*60)
        
        efficiency_result = await benchmark_runner.run_resource_efficiency_test(duration_seconds=30)
        all_results["resource_efficiency"] = efficiency_result
        
        # ç”ŸæˆåŸºå‡†æŠ¥å‘Š
        print("\n" + "="*80)
        print("åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        generate_benchmark_report(all_results)
        
        # ç”Ÿæˆæ€§èƒ½å›¾è¡¨
        try:
            generate_performance_charts(all_results)
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"åŸºå‡†æµ‹è¯•é”™è¯¯: {e}")
    
    finally:
        # æ¸…ç†èµ„æº
        print("\nğŸ§¹ æ¸…ç†æµ‹è¯•èµ„æº...")
        try:
            await integrator.shutdown_system()
            print("âœ… ç³»ç»Ÿå…³é—­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  ç³»ç»Ÿå…³é—­æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    print("\n" + "="*80)
    print("æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
    print("="*80)


def generate_benchmark_report(results: Dict[str, Any]):
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
    
    print("\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("-" * 70)
    
    # å•ä»»åŠ¡æ€§èƒ½
    if "single_task" in results and "error" not in results["single_task"]:
        single = results["single_task"]
        print(f"å•ä»»åŠ¡å¤„ç†æ€§èƒ½:")
        print(f"  â€¢ å¹³å‡å“åº”æ—¶é—´: {single['response_time']['avg']:.3f} ç§’")
        print(f"  â€¢ P95å“åº”æ—¶é—´: {single['response_time']['p95']:.3f} ç§’")
        print(f"  â€¢ P99å“åº”æ—¶é—´: {single['response_time']['p99']:.3f} ç§’")
        print(f"  â€¢ æˆåŠŸç‡: {single['successful']/single['iterations']:.2%}")
    
    # æ‰¹é‡å¤„ç†æ€§èƒ½
    if "batch_processing" in results:
        batch = results["batch_processing"]
        print(f"\næ‰¹é‡å¤„ç†æ€§èƒ½:")
        for batch_size, result in batch.items():
            if isinstance(result, dict):
                print(f"  â€¢ æ‰¹æ¬¡å¤§å° {batch_size}: {result['throughput']:.2f} ä»»åŠ¡/ç§’")
    
    # å·¥ä½œæµæ¨¡æ¿å¯¹æ¯”
    if "workflow_templates" in results:
        templates = results["workflow_templates"]
        print(f"\nå·¥ä½œæµæ¨¡æ¿æ€§èƒ½å¯¹æ¯”:")
        for template, result in templates.items():
            if isinstance(result, dict) and "error" not in result:
                print(f"  â€¢ {template}: {result['avg_response_time']:.3f} ç§’ (æˆåŠŸç‡: {result['success_rate']:.2%})")
    
    # ç¼“å­˜æ€§èƒ½
    if "cache_performance" in results and "error" not in results["cache_performance"]:
        cache = results["cache_performance"]
        print(f"\nç¼“å­˜æ€§èƒ½:")
        for cache_size, result in cache.items():
            if isinstance(result, dict):
                print(f"  â€¢ å¤§å° {cache_size}: è¯»å– {result['read_ops_per_sec']:.0f} ops/sec, "
                      f"å†™å…¥ {result['write_ops_per_sec']:.0f} ops/sec")
    
    # èµ„æºæ•ˆç‡
    if "resource_efficiency" in results and "error" not in results["resource_efficiency"]:
        efficiency = results["resource_efficiency"]
        print(f"\nèµ„æºä½¿ç”¨æ•ˆç‡:")
        print(f"  â€¢ ä»»åŠ¡å¤„ç†é€Ÿç‡: {efficiency['tasks_per_second']:.2f} ä»»åŠ¡/ç§’")
        print(f"  â€¢ å¹³å‡CPUä½¿ç”¨: {efficiency['resource_usage']['avg_cpu']:.1f}%")
        print(f"  â€¢ å¹³å‡å†…å­˜ä½¿ç”¨: {efficiency['resource_usage']['avg_memory']:.1f}%")
        print(f"  â€¢ æ•ˆç‡åˆ†æ•°: {efficiency['efficiency_score']:.2f}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"benchmark_report_{timestamp}.json"
    
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str, ensure_ascii=False)
        print(f"\nğŸ“„ è¯¦ç»†åŸºå‡†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def generate_performance_charts(results: Dict[str, Any]):
    """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
    
    try:
        import matplotlib.pyplot as plt
        import numpy as np
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('LangGraphå¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•', fontsize=16)
        
        # 1. æ‰¹é‡å¤„ç†ååé‡å›¾è¡¨
        if "batch_processing" in results:
            batch = results["batch_processing"]
            batch_sizes = []
            throughputs = []
            
            for batch_size, result in batch.items():
                if isinstance(result, dict):
                    batch_sizes.append(batch_size)
                    throughputs.append(result['throughput'])
            
            if batch_sizes:
                axes[0, 0].plot(batch_sizes, throughputs, 'b-o')
                axes[0, 0].set_title('æ‰¹é‡å¤„ç†ååé‡')
                axes[0, 0].set_xlabel('æ‰¹æ¬¡å¤§å°')
                axes[0, 0].set_ylabel('ååé‡ (ä»»åŠ¡/ç§’)')
                axes[0, 0].grid(True)
        
        # 2. å·¥ä½œæµæ¨¡æ¿å“åº”æ—¶é—´å¯¹æ¯”
        if "workflow_templates" in results:
            templates = results["workflow_templates"]
            template_names = []
            response_times = []
            
            for template, result in templates.items():
                if isinstance(result, dict) and "error" not in result:
                    template_names.append(template)
                    response_times.append(result['avg_response_time'])
            
            if template_names:
                axes[0, 1].bar(template_names, response_times)
                axes[0, 1].set_title('å·¥ä½œæµæ¨¡æ¿å“åº”æ—¶é—´å¯¹æ¯”')
                axes[0, 1].set_xlabel('æ¨¡æ¿ç±»å‹')
                axes[0, 1].set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')
                axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. ç¼“å­˜æ€§èƒ½å›¾è¡¨
        if "cache_performance" in results and "error" not in results["cache_performance"]:
            cache = results["cache_performance"]
            cache_sizes = []
            read_speeds = []
            write_speeds = []
            
            for cache_size, result in cache.items():
                if isinstance(result, dict):
                    cache_sizes.append(cache_size)
                    read_speeds.append(result['read_ops_per_sec'])
                    write_speeds.append(result['write_ops_per_sec'])
            
            if cache_sizes:
                axes[1, 0].plot(cache_sizes, read_speeds, 'g-o', label='è¯»å–')
                axes[1, 0].plot(cache_sizes, write_speeds, 'r-o', label='å†™å…¥')
                axes[1, 0].set_title('ç¼“å­˜æ€§èƒ½')
                axes[1, 0].set_xlabel('ç¼“å­˜å¤§å°')
                axes[1, 0].set_ylabel('æ“ä½œé€Ÿåº¦ (ops/sec)')
                axes[1, 0].legend()
                axes[1, 0].grid(True)
        
        # 4. å•ä»»åŠ¡å“åº”æ—¶é—´åˆ†å¸ƒ
        if "single_task" in results and "error" not in results["single_task"]:
            single = results["single_task"]
            response_time_data = [
                single['response_time']['min'],
                single['response_time']['median'],
                single['response_time']['avg'],
                single['response_time']['p95'],
                single['response_time']['p99'],
                single['response_time']['max']
            ]
            labels = ['æœ€å°å€¼', 'ä¸­ä½æ•°', 'å¹³å‡å€¼', 'P95', 'P99', 'æœ€å¤§å€¼']
            
            axes[1, 1].bar(labels, response_time_data)
            axes[1, 1].set_title('å•ä»»åŠ¡å“åº”æ—¶é—´åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('ç»Ÿè®¡æŒ‡æ ‡')
            axes[1, 1].set_ylabel('å“åº”æ—¶é—´ (ç§’)')
            axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chart_file = f"benchmark_charts_{timestamp}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        
        print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_file}")
        
        # æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¦‚æœåœ¨äº¤äº’ç¯å¢ƒä¸­ï¼‰
        # plt.show()
        
    except ImportError:
        print("âš ï¸  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        asyncio.run(run_comprehensive_benchmark())
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"åŸºå‡†æµ‹è¯•ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {e}")


if __name__ == "__main__":
    main()