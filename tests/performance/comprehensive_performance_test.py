#!/usr/bin/env python3
"""
ç»¼åˆæ€§èƒ½å’Œå‹åŠ›æµ‹è¯•æ‰§è¡Œå™¨

è¿™ä¸ªè„šæœ¬æ‰§è¡Œä»»åŠ¡7.4çš„æ‰€æœ‰æµ‹è¯•è¦æ±‚ï¼š
- è¿›è¡Œå¹¶å‘ä»»åŠ¡å¤„ç†çš„æ€§èƒ½æµ‹è¯•
- æ‰§è¡Œå¤§è§„æ¨¡å·¥ä½œæµçš„å‹åŠ›æµ‹è¯•  
- æµ‹è¯•MVP2å‰ç«¯é›†æˆçš„ç¨³å®šæ€§

é›†æˆæ‰€æœ‰ç°æœ‰çš„æµ‹è¯•æ¨¡å—å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Šã€‚
"""

import asyncio
import logging
import time
import json
import subprocess
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from langgraph_multi_agent.system.integration import SystemIntegrator
from langgraph_multi_agent.api.app import create_app
import uvicorn
import threading
import multiprocessing


class ComprehensivePerformanceTestRunner:
    """ç»¼åˆæ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        self.api_server_process = None
        self.api_server_thread = None
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "api_host": "localhost",
            "api_port": 8001,  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
            "test_duration_minutes": 10,
            "concurrent_users": 50,
            "stress_test_tasks": 200,
            "mvp2_test_requests": 300
        }
        
        logger.info("ç»¼åˆæ€§èƒ½æµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•"""
        
        print("=" * 100)
        print("LangGraphå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ - ç»¼åˆæ€§èƒ½å’Œå‹åŠ›æµ‹è¯• (ä»»åŠ¡7.4)")
        print("=" * 100)
        
        self.start_time = datetime.now()
        
        try:
            # 1. å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
            await self._setup_test_environment()
            
            # 2. æ‰§è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•
            await self._run_benchmark_tests()
            
            # 3. æ‰§è¡Œå‹åŠ›æµ‹è¯•
            await self._run_stress_tests()
            
            # 4. æ‰§è¡ŒMVP2é›†æˆæµ‹è¯•
            await self._run_mvp2_integration_tests()
            
            # 5. æ‰§è¡Œå¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•
            await self._run_large_scale_workflow_tests()
            
            # 6. æ‰§è¡Œå¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•
            await self._run_concurrent_task_tests()
            
            # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            self._generate_comprehensive_report()
            
        except Exception as e:
            logger.error(f"ç»¼åˆæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            self.test_results["error"] = str(e)
        
        finally:
            # 8. æ¸…ç†æµ‹è¯•ç¯å¢ƒ
            await self._cleanup_test_environment()
        
        self.end_time = datetime.now()
        
        print("\n" + "=" * 100)
        print("ç»¼åˆæ€§èƒ½å’Œå‹åŠ›æµ‹è¯•å®Œæˆ")
        print("=" * 100)
        
        return self.test_results
    
    async def _setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("\nğŸš€ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # åˆå§‹åŒ–ç³»ç»Ÿé›†æˆå™¨
            config = {
                "checkpoint_storage": "memory",
                "enable_metrics": True,
                "enable_tracing": False,
                "optimization_level": "aggressive",
                "performance": {
                    "max_cache_size": 20000,
                    "enable_auto_optimization": True,
                    "max_workers": 8,
                    "execution_mode": "adaptive",
                    "enable_auto_scaling": True
                }
            }
            
            self.integrator = SystemIntegrator(config)
            success = await self.integrator.initialize_system()
            
            if not success:
                raise Exception("ç³»ç»Ÿé›†æˆå™¨åˆå§‹åŒ–å¤±è´¥")
            
            # å¯åŠ¨APIæœåŠ¡å™¨
            await self._start_api_server()
            
            # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            await asyncio.sleep(3)
            
            print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
            raise
    
    async def _start_api_server(self):
        """å¯åŠ¨APIæœåŠ¡å™¨"""
        try:
            def run_server():
                app = create_app()
                uvicorn.run(
                    app,
                    host=self.test_config["api_host"],
                    port=self.test_config["api_port"],
                    log_level="warning",  # å‡å°‘æ—¥å¿—è¾“å‡º
                    access_log=False
                )
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¯åŠ¨æœåŠ¡å™¨
            self.api_server_thread = threading.Thread(target=run_server, daemon=True)
            self.api_server_thread.start()
            
            logger.info(f"APIæœåŠ¡å™¨å¯åŠ¨åœ¨ {self.test_config['api_host']}:{self.test_config['api_port']}")
            
        except Exception as e:
            logger.error(f"APIæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    async def _run_benchmark_tests(self):
        """è¿è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•"""
        print("\n" + "="*80)
        print("æµ‹è¯•é˜¶æ®µ 1/5: åŸºå‡†æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        try:
            # è¿è¡Œç°æœ‰çš„åŸºå‡†æµ‹è¯•
            result = await self._execute_test_script("benchmark.py")
            self.test_results["benchmark_tests"] = result
            
            if result.get("success"):
                print("âœ… åŸºå‡†æ€§èƒ½æµ‹è¯•å®Œæˆ")
                self._print_benchmark_summary(result)
            else:
                print("âŒ åŸºå‡†æ€§èƒ½æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["benchmark_tests"] = {"error": str(e)}
    
    async def _run_stress_tests(self):
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        print("\n" + "="*80)
        print("æµ‹è¯•é˜¶æ®µ 2/5: ç³»ç»Ÿå‹åŠ›æµ‹è¯•")
        print("="*80)
        
        try:
            # è¿è¡Œç°æœ‰çš„å‹åŠ›æµ‹è¯•
            result = await self._execute_test_script("stress_test.py")
            self.test_results["stress_tests"] = result
            
            if result.get("success"):
                print("âœ… ç³»ç»Ÿå‹åŠ›æµ‹è¯•å®Œæˆ")
                self._print_stress_summary(result)
            else:
                print("âŒ ç³»ç»Ÿå‹åŠ›æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["stress_tests"] = {"error": str(e)}
    
    async def _run_mvp2_integration_tests(self):
        """è¿è¡ŒMVP2é›†æˆæµ‹è¯•"""
        print("\n" + "="*80)
        print("æµ‹è¯•é˜¶æ®µ 3/5: MVP2å‰ç«¯é›†æˆç¨³å®šæ€§æµ‹è¯•")
        print("="*80)
        
        try:
            # è¿è¡ŒMVP2é›†æˆæµ‹è¯•
            result = await self._execute_test_script("mvp2_integration_test.py")
            self.test_results["mvp2_integration_tests"] = result
            
            if result.get("success"):
                print("âœ… MVP2é›†æˆæµ‹è¯•å®Œæˆ")
                self._print_mvp2_summary(result)
            else:
                print("âŒ MVP2é›†æˆæµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"MVP2é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results["mvp2_integration_tests"] = {"error": str(e)}
    
    async def _run_large_scale_workflow_tests(self):
        """è¿è¡Œå¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•"""
        print("\n" + "="*80)
        print("æµ‹è¯•é˜¶æ®µ 4/5: å¤§è§„æ¨¡å·¥ä½œæµå‹åŠ›æµ‹è¯•")
        print("="*80)
        
        try:
            # åˆ›å»ºå¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•
            result = await self._execute_large_scale_workflow_test()
            self.test_results["large_scale_workflow_tests"] = result
            
            print("âœ… å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•å®Œæˆ")
            self._print_workflow_summary(result)
                
        except Exception as e:
            logger.error(f"å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
            self.test_results["large_scale_workflow_tests"] = {"error": str(e)}
    
    async def _run_concurrent_task_tests(self):
        """è¿è¡Œå¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•"""
        print("\n" + "="*80)
        print("æµ‹è¯•é˜¶æ®µ 5/5: å¹¶å‘ä»»åŠ¡å¤„ç†æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        try:
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡æµ‹è¯•
            result = await self._execute_concurrent_task_test()
            self.test_results["concurrent_task_tests"] = result
            
            print("âœ… å¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•å®Œæˆ")
            self._print_concurrent_summary(result)
                
        except Exception as e:
            logger.error(f"å¹¶å‘ä»»åŠ¡æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["concurrent_task_tests"] = {"error": str(e)}
    
    async def _execute_test_script(self, script_name: str) -> Dict[str, Any]:
        """æ‰§è¡Œæµ‹è¯•è„šæœ¬"""
        try:
            script_path = Path(__file__).parent / script_name
            
            if not script_path.exists():
                return {"success": False, "error": f"æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨: {script_name}"}
            
            # ä½¿ç”¨subprocessè¿è¡Œæµ‹è¯•è„šæœ¬
            process = await asyncio.create_subprocess_exec(
                sys.executable, str(script_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            result = {
                "success": process.returncode == 0,
                "stdout": stdout.decode('utf-8', errors='ignore'),
                "stderr": stderr.decode('utf-8', errors='ignore'),
                "return_code": process.returncode
            }
            
            # å°è¯•è§£æJSONè¾“å‡º
            try:
                if "JSON_RESULT:" in result["stdout"]:
                    json_start = result["stdout"].find("JSON_RESULT:") + 12
                    json_data = result["stdout"][json_start:].strip()
                    result["data"] = json.loads(json_data)
            except json.JSONDecodeError:
                pass
            
            return result
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _execute_large_scale_workflow_test(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•"""
        try:
            print("   åˆ›å»ºå¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•...")
            
            # æµ‹è¯•å‚æ•°
            num_workflows = 100
            tasks_per_workflow = 5
            concurrent_workflows = 20
            
            start_time = time.time()
            successful_workflows = 0
            failed_workflows = 0
            workflow_times = []
            
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(concurrent_workflows)
            
            async def execute_workflow(workflow_id: int):
                async with semaphore:
                    workflow_start = time.time()
                    try:
                        # åˆ›å»ºå·¥ä½œæµ
                        workflow_name = f"large_scale_workflow_{workflow_id}"
                        await self.integrator.create_workflow(workflow_name, "complex")
                        
                        # åˆ›å»ºä»»åŠ¡æ•°æ®
                        task_data = {
                            "task_id": f"large_task_{workflow_id}",
                            "title": f"å¤§è§„æ¨¡æµ‹è¯•ä»»åŠ¡ {workflow_id}",
                            "description": f"å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯• - å·¥ä½œæµ {workflow_id}",
                            "requirements": [f"å¤„ç†æ­¥éª¤ {i}" for i in range(tasks_per_workflow)]
                        }
                        
                        # æ‰§è¡Œå·¥ä½œæµ
                        result = await self.integrator.execute_workflow(workflow_name, task_data)
                        
                        workflow_time = time.time() - workflow_start
                        workflow_times.append(workflow_time)
                        
                        return {"success": True, "time": workflow_time}
                        
                    except Exception as e:
                        workflow_time = time.time() - workflow_start
                        workflow_times.append(workflow_time)
                        logger.error(f"å·¥ä½œæµ {workflow_id} æ‰§è¡Œå¤±è´¥: {e}")
                        return {"success": False, "time": workflow_time, "error": str(e)}
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥ä½œæµ
            print(f"   æ‰§è¡Œ {num_workflows} ä¸ªå·¥ä½œæµï¼Œå¹¶å‘åº¦ {concurrent_workflows}...")
            
            tasks = [execute_workflow(i) for i in range(num_workflows)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if isinstance(result, dict) and result.get("success"):
                    successful_workflows += 1
                else:
                    failed_workflows += 1
            
            total_time = time.time() - start_time
            
            return {
                "success": True,
                "total_workflows": num_workflows,
                "successful_workflows": successful_workflows,
                "failed_workflows": failed_workflows,
                "success_rate": successful_workflows / num_workflows,
                "total_time": total_time,
                "throughput": num_workflows / total_time,
                "avg_workflow_time": sum(workflow_times) / len(workflow_times) if workflow_times else 0,
                "min_workflow_time": min(workflow_times) if workflow_times else 0,
                "max_workflow_time": max(workflow_times) if workflow_times else 0,
                "concurrent_workflows": concurrent_workflows,
                "tasks_per_workflow": tasks_per_workflow
            }
            
        except Exception as e:
            logger.error(f"å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_concurrent_task_test(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•"""
        try:
            print("   åˆ›å»ºå¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•...")
            
            # æµ‹è¯•å‚æ•°
            num_tasks = 200
            concurrent_tasks = 50
            task_types = ["simple", "analysis", "complex"]
            
            start_time = time.time()
            successful_tasks = 0
            failed_tasks = 0
            task_times = []
            
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(concurrent_tasks)
            
            async def execute_task(task_id: int):
                async with semaphore:
                    task_start = time.time()
                    try:
                        # éšæœºé€‰æ‹©ä»»åŠ¡ç±»å‹
                        task_type = task_types[task_id % len(task_types)]
                        
                        # åˆ›å»ºå·¥ä½œæµ
                        workflow_name = f"concurrent_workflow_{task_id}"
                        await self.integrator.create_workflow(workflow_name, task_type)
                        
                        # åˆ›å»ºä»»åŠ¡æ•°æ®
                        task_data = {
                            "task_id": f"concurrent_task_{task_id}",
                            "title": f"å¹¶å‘æµ‹è¯•ä»»åŠ¡ {task_id}",
                            "description": f"å¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯• - ä»»åŠ¡ {task_id}",
                            "task_type": task_type,
                            "priority": (task_id % 4) + 1,
                            "requirements": ["å¹¶å‘å¤„ç†", "æ€§èƒ½æµ‹è¯•"]
                        }
                        
                        # æ‰§è¡Œä»»åŠ¡
                        result = await self.integrator.execute_workflow(workflow_name, task_data)
                        
                        task_time = time.time() - task_start
                        task_times.append(task_time)
                        
                        return {"success": True, "time": task_time, "task_type": task_type}
                        
                    except Exception as e:
                        task_time = time.time() - task_start
                        task_times.append(task_time)
                        logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
                        return {"success": False, "time": task_time, "error": str(e)}
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            print(f"   æ‰§è¡Œ {num_tasks} ä¸ªä»»åŠ¡ï¼Œå¹¶å‘åº¦ {concurrent_tasks}...")
            
            tasks = [execute_task(i) for i in range(num_tasks)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            task_type_stats = {task_type: {"success": 0, "failed": 0} for task_type in task_types}
            
            for result in results:
                if isinstance(result, dict):
                    if result.get("success"):
                        successful_tasks += 1
                        task_type = result.get("task_type", "unknown")
                        if task_type in task_type_stats:
                            task_type_stats[task_type]["success"] += 1
                    else:
                        failed_tasks += 1
                        task_type = result.get("task_type", "unknown")
                        if task_type in task_type_stats:
                            task_type_stats[task_type]["failed"] += 1
                else:
                    failed_tasks += 1
            
            total_time = time.time() - start_time
            
            return {
                "success": True,
                "total_tasks": num_tasks,
                "successful_tasks": successful_tasks,
                "failed_tasks": failed_tasks,
                "success_rate": successful_tasks / num_tasks,
                "total_time": total_time,
                "throughput": num_tasks / total_time,
                "avg_task_time": sum(task_times) / len(task_times) if task_times else 0,
                "min_task_time": min(task_times) if task_times else 0,
                "max_task_time": max(task_times) if task_times else 0,
                "concurrent_tasks": concurrent_tasks,
                "task_type_stats": task_type_stats
            }
            
        except Exception as e:
            logger.error(f"å¹¶å‘ä»»åŠ¡æµ‹è¯•å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _print_benchmark_summary(self, result: Dict[str, Any]):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ‘˜è¦"""
        if "data" in result:
            data = result["data"]
            print(f"   å•ä»»åŠ¡å¹³å‡å“åº”æ—¶é—´: {data.get('single_task', {}).get('response_time', {}).get('avg', 0):.3f} ç§’")
            print(f"   æ‰¹é‡å¤„ç†æœ€å¤§ååé‡: {max(data.get('batch_processing', {}).values(), key=lambda x: x.get('throughput', 0) if isinstance(x, dict) else 0, default={}).get('throughput', 0):.2f} ä»»åŠ¡/ç§’")
    
    def _print_stress_summary(self, result: Dict[str, Any]):
        """æ‰“å°å‹åŠ›æµ‹è¯•æ‘˜è¦"""
        if "data" in result:
            data = result["data"]
            concurrent = data.get("concurrent_tasks", {})
            print(f"   å¹¶å‘æµ‹è¯•æˆåŠŸç‡: {concurrent.get('success_rate', 0):.2%}")
            print(f"   ç³»ç»Ÿååé‡: {concurrent.get('throughput', 0):.2f} ä»»åŠ¡/ç§’")
    
    def _print_mvp2_summary(self, result: Dict[str, Any]):
        """æ‰“å°MVP2æµ‹è¯•æ‘˜è¦"""
        if "data" in result:
            data = result["data"]
            api = data.get("api_stability", {})
            print(f"   APIç¨³å®šæ€§: {api.get('success_rate', 0):.2%}")
            print(f"   WebSocketç¨³å®šæ€§: {data.get('websocket_stability', {}).get('message_success_rate', 0):.2%}")
    
    def _print_workflow_summary(self, result: Dict[str, Any]):
        """æ‰“å°å·¥ä½œæµæµ‹è¯•æ‘˜è¦"""
        print(f"   å·¥ä½œæµæˆåŠŸç‡: {result.get('success_rate', 0):.2%}")
        print(f"   å·¥ä½œæµååé‡: {result.get('throughput', 0):.2f} å·¥ä½œæµ/ç§’")
        print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {result.get('avg_workflow_time', 0):.3f} ç§’")
    
    def _print_concurrent_summary(self, result: Dict[str, Any]):
        """æ‰“å°å¹¶å‘æµ‹è¯•æ‘˜è¦"""
        print(f"   ä»»åŠ¡æˆåŠŸç‡: {result.get('success_rate', 0):.2%}")
        print(f"   ä»»åŠ¡ååé‡: {result.get('throughput', 0):.2f} ä»»åŠ¡/ç§’")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {result.get('avg_task_time', 0):.3f} ç§’")
    
    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("\n" + "="*100)
        print("ç»¼åˆæ€§èƒ½å’Œå‹åŠ›æµ‹è¯•æŠ¥å‘Š (ä»»åŠ¡7.4)")
        print("="*100)
        
        # æµ‹è¯•æ¦‚è§ˆ
        total_duration = (self.end_time - self.start_time).total_seconds() if self.end_time and self.start_time else 0
        
        print(f"\nğŸ“Š æµ‹è¯•æ¦‚è§ˆ:")
        print(f"  â€¢ æµ‹è¯•å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S') if self.start_time else 'N/A'}")
        print(f"  â€¢ æµ‹è¯•ç»“æŸæ—¶é—´: {self.end_time.strftime('%Y-%m-%d %H:%M:%S') if self.end_time else 'N/A'}")
        print(f"  â€¢ æ€»æµ‹è¯•æ—¶é•¿: {total_duration:.1f} ç§’")
        print(f"  â€¢ å®Œæˆæµ‹è¯•æ¨¡å—: {len([k for k, v in self.test_results.items() if isinstance(v, dict) and v.get('success')])}/5")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“ˆ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        print("-" * 80)
        
        # 1. åŸºå‡†æ€§èƒ½æµ‹è¯•ç»“æœ
        benchmark = self.test_results.get("benchmark_tests", {})
        if benchmark.get("success"):
            print("âœ… åŸºå‡†æ€§èƒ½æµ‹è¯•: é€šè¿‡")
        else:
            print("âŒ åŸºå‡†æ€§èƒ½æµ‹è¯•: å¤±è´¥")
            if "error" in benchmark:
                print(f"    é”™è¯¯: {benchmark['error']}")
        
        # 2. å‹åŠ›æµ‹è¯•ç»“æœ
        stress = self.test_results.get("stress_tests", {})
        if stress.get("success"):
            print("âœ… ç³»ç»Ÿå‹åŠ›æµ‹è¯•: é€šè¿‡")
        else:
            print("âŒ ç³»ç»Ÿå‹åŠ›æµ‹è¯•: å¤±è´¥")
            if "error" in stress:
                print(f"    é”™è¯¯: {stress['error']}")
        
        # 3. MVP2é›†æˆæµ‹è¯•ç»“æœ
        mvp2 = self.test_results.get("mvp2_integration_tests", {})
        if mvp2.get("success"):
            print("âœ… MVP2å‰ç«¯é›†æˆæµ‹è¯•: é€šè¿‡")
        else:
            print("âŒ MVP2å‰ç«¯é›†æˆæµ‹è¯•: å¤±è´¥")
            if "error" in mvp2:
                print(f"    é”™è¯¯: {mvp2['error']}")
        
        # 4. å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•ç»“æœ
        workflow = self.test_results.get("large_scale_workflow_tests", {})
        if workflow.get("success"):
            print("âœ… å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•: é€šè¿‡")
            print(f"    æˆåŠŸç‡: {workflow.get('success_rate', 0):.2%}")
            print(f"    ååé‡: {workflow.get('throughput', 0):.2f} å·¥ä½œæµ/ç§’")
        else:
            print("âŒ å¤§è§„æ¨¡å·¥ä½œæµæµ‹è¯•: å¤±è´¥")
            if "error" in workflow:
                print(f"    é”™è¯¯: {workflow['error']}")
        
        # 5. å¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•ç»“æœ
        concurrent = self.test_results.get("concurrent_task_tests", {})
        if concurrent.get("success"):
            print("âœ… å¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•: é€šè¿‡")
            print(f"    æˆåŠŸç‡: {concurrent.get('success_rate', 0):.2%}")
            print(f"    ååé‡: {concurrent.get('throughput', 0):.2f} ä»»åŠ¡/ç§’")
        else:
            print("âŒ å¹¶å‘ä»»åŠ¡å¤„ç†æµ‹è¯•: å¤±è´¥")
            if "error" in concurrent:
                print(f"    é”™è¯¯: {concurrent['error']}")
        
        # ç»¼åˆè¯„ä¼°
        print(f"\nğŸ¯ ä»»åŠ¡7.4å®Œæˆæƒ…å†µè¯„ä¼°:")
        print("-" * 50)
        
        completed_requirements = []
        
        # æ£€æŸ¥å¹¶å‘ä»»åŠ¡å¤„ç†æ€§èƒ½æµ‹è¯•
        if concurrent.get("success"):
            completed_requirements.append("âœ… è¿›è¡Œå¹¶å‘ä»»åŠ¡å¤„ç†çš„æ€§èƒ½æµ‹è¯•")
        else:
            completed_requirements.append("âŒ è¿›è¡Œå¹¶å‘ä»»åŠ¡å¤„ç†çš„æ€§èƒ½æµ‹è¯•")
        
        # æ£€æŸ¥å¤§è§„æ¨¡å·¥ä½œæµå‹åŠ›æµ‹è¯•
        if workflow.get("success") and stress.get("success"):
            completed_requirements.append("âœ… æ‰§è¡Œå¤§è§„æ¨¡å·¥ä½œæµçš„å‹åŠ›æµ‹è¯•")
        else:
            completed_requirements.append("âŒ æ‰§è¡Œå¤§è§„æ¨¡å·¥ä½œæµçš„å‹åŠ›æµ‹è¯•")
        
        # æ£€æŸ¥MVP2å‰ç«¯é›†æˆç¨³å®šæ€§æµ‹è¯•
        if mvp2.get("success"):
            completed_requirements.append("âœ… æµ‹è¯•MVP2å‰ç«¯é›†æˆçš„ç¨³å®šæ€§")
        else:
            completed_requirements.append("âŒ æµ‹è¯•MVP2å‰ç«¯é›†æˆçš„ç¨³å®šæ€§")
        
        for requirement in completed_requirements:
            print(f"  {requirement}")
        
        # è®¡ç®—å®Œæˆåº¦
        completed_count = sum(1 for req in completed_requirements if req.startswith("âœ…"))
        completion_rate = completed_count / len(completed_requirements)
        
        print(f"\nä»»åŠ¡7.4å®Œæˆåº¦: {completed_count}/{len(completed_requirements)} ({completion_rate:.1%})")
        
        if completion_rate == 1.0:
            print("ğŸ† ä»»åŠ¡7.4å·²å®Œå…¨å®Œæˆï¼")
        elif completion_rate >= 0.67:
            print("ğŸ‘ ä»»åŠ¡7.4åŸºæœ¬å®Œæˆï¼Œéƒ¨åˆ†æµ‹è¯•éœ€è¦ä¼˜åŒ–")
        else:
            print("âš ï¸ ä»»åŠ¡7.4éœ€è¦è¿›ä¸€æ­¥å®Œå–„")
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_comprehensive_report()
    
    def _save_comprehensive_report(self):
        """ä¿å­˜ç»¼åˆæŠ¥å‘Š"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = f"comprehensive_performance_report_{timestamp}.json"
            
            report_data = {
                "test_metadata": {
                    "start_time": self.start_time.isoformat() if self.start_time else None,
                    "end_time": self.end_time.isoformat() if self.end_time else None,
                    "total_duration_seconds": (self.end_time - self.start_time).total_seconds() if self.end_time and self.start_time else 0,
                    "test_config": self.test_config
                },
                "test_results": self.test_results,
                "task_7_4_requirements": {
                    "concurrent_task_processing": self.test_results.get("concurrent_task_tests", {}).get("success", False),
                    "large_scale_workflow_stress": self.test_results.get("large_scale_workflow_tests", {}).get("success", False) and self.test_results.get("stress_tests", {}).get("success", False),
                    "mvp2_frontend_integration": self.test_results.get("mvp2_integration_tests", {}).get("success", False)
                }
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, default=str, ensure_ascii=False)
            
            print(f"\nğŸ“„ ç»¼åˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    async def _cleanup_test_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        print("\nğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # å…³é—­ç³»ç»Ÿé›†æˆå™¨
            if hasattr(self, 'integrator'):
                await self.integrator.shutdown_system()
            
            # APIæœåŠ¡å™¨ä¼šåœ¨ä¸»çº¿ç¨‹ç»“æŸæ—¶è‡ªåŠ¨å…³é—­
            
            print("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•ç¯å¢ƒæ¸…ç†å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        runner = ComprehensivePerformanceTestRunner()
        results = await runner.run_all_tests()
        
        # è¿”å›ç»“æœç”¨äºéªŒè¯
        return results
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return {"interrupted": True}
    except Exception as e:
        print(f"ç»¼åˆæ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {e}")
        return {"error": str(e)}


if __name__ == "__main__":
    asyncio.run(main())