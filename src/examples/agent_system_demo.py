"""
æ™ºèƒ½ä½“ç³»ç»Ÿæ¼”ç¤º - ä¸ªäººAIåŠ©æ‰‹ä»»åŠ¡

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å®Œæ•´çš„æ™ºèƒ½ä½“ç³»ç»Ÿå·¥ä½œæµç¨‹ï¼š
1. MetaAgent åˆ†æä»»åŠ¡å¤æ‚åº¦
2. TaskDecomposer åˆ†è§£ä»»åŠ¡ä¸ºå­ä»»åŠ¡
3. CoordinatorAgent åè°ƒæ‰§è¡Œ
4. æ±‡æ€»ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, Any, List
import uuid

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AgentSystemDemo:
    """æ™ºèƒ½ä½“ç³»ç»Ÿæ¼”ç¤ºç±»"""

    def __init__(self):
        self.results_dir = Path("demo_results")
        self.results_dir.mkdir(exist_ok=True)
        self.execution_log = []
        self.start_time = datetime.now(timezone.utc)

    def log_execution(self, agent_type: str, action: str, data: Any = None):
        """è®°å½•æ‰§è¡Œæ—¥å¿—"""
        timestamp = datetime.now(timezone.utc)
        log_entry = {
            "timestamp": timestamp.isoformat(),
            "agent_type": agent_type,
            "action": action,
            "data": data
        }
        self.execution_log.append(log_entry)
        logger.info(f"[{agent_type}] {action}")

    async def run_demo(self, task_file: str):
        """è¿è¡Œå®Œæ•´çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ¼”ç¤º"""
        logger.info("å¼€å§‹æ‰§è¡Œä¸ªäººAIåŠ©æ‰‹ä»»åŠ¡...")

        # åŠ è½½ä»»åŠ¡
        task_data = self._load_task(task_file)
        self.log_execution("System", "TaskLoaded", {"task_id": task_data.get("task_id")})

        # æ‰§è¡Œå®Œæ•´æµç¨‹
        final_result = await self._execute_complete_workflow(task_data)

        # ä¿å­˜ç»“æœ
        await self._save_results(final_result)

        return final_result

    def _load_task(self, task_file: str) -> Dict[str, Any]:
        """åŠ è½½ä»»åŠ¡æ•°æ®"""
        try:
            with open(task_file, 'r', encoding='utf-8') as f:
                task_data = json.load(f)
            logger.info(f"ä»»åŠ¡åŠ è½½æˆåŠŸ: {task_data.get('title')}")
            return task_data
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡åŠ è½½å¤±è´¥: {e}")
            raise

    async def _execute_complete_workflow(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹"""
        workflow_result = {
            "task_id": task_data["task_id"],
            "task_title": task_data["title"],
            "execution_id": str(uuid.uuid4()),
            "start_time": self.start_time.isoformat(),
            "phases": []
        }

        try:
            # é˜¶æ®µ1: MetaAgent ä»»åŠ¡åˆ†æ
            self.log_execution("System", "Phase1_Start", "MetaAgentä»»åŠ¡åˆ†æ")
            meta_result = await self._simulate_meta_agent_analysis(task_data)
            workflow_result["phases"].append({
                "phase": "meta_analysis",
                "status": "completed",
                "result": meta_result,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

            # é˜¶æ®µ2: TaskDecomposer ä»»åŠ¡åˆ†è§£
            self.log_execution("System", "Phase2_Start", "TaskDecomposerä»»åŠ¡åˆ†è§£")
            decomposition_result = await self._simulate_task_decomposition(task_data, meta_result)
            workflow_result["phases"].append({
                "phase": "task_decomposition",
                "status": "completed",
                "result": decomposition_result,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

            # é˜¶æ®µ3: CoordinatorAgent ä»»åŠ¡åè°ƒ
            self.log_execution("System", "Phase3_Start", "CoordinatorAgentä»»åŠ¡åè°ƒ")
            coordination_result = await self._simulate_coordination(decomposition_result)
            workflow_result["phases"].append({
                "phase": "coordination",
                "status": "completed",
                "result": coordination_result,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

            # é˜¶æ®µ4: ä»»åŠ¡æ‰§è¡Œæ¨¡æ‹Ÿ
            self.log_execution("System", "Phase4_Start", "ä»»åŠ¡æ‰§è¡Œæ¨¡æ‹Ÿ")
            execution_result = await self._simulate_task_execution(coordination_result)
            workflow_result["phases"].append({
                "phase": "execution",
                "status": "completed",
                "result": execution_result,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

            # é˜¶æ®µ5: ç»“æœæ±‡æ€»
            self.log_execution("System", "Phase5_Start", "ç»“æœæ±‡æ€»")
            summary_result = await self._generate_summary(workflow_result)
            workflow_result["phases"].append({
                "phase": "summary",
                "status": "completed",
                "result": summary_result,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

            workflow_result["end_time"] = datetime.now(timezone.utc).isoformat()
            workflow_result["duration_seconds"] = (
                datetime.fromisoformat(workflow_result["end_time"]) -
                datetime.fromisoformat(workflow_result["start_time"])
            ).total_seconds()
            workflow_result["status"] = "success"
            workflow_result["success"] = True

            logger.info(f"å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {workflow_result['duration_seconds']:.2f}ç§’")

        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            workflow_result["status"] = "failed"
            workflow_result["success"] = False
            workflow_result["error"] = str(e)

        return workflow_result

    async def _simulate_meta_agent_analysis(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸMetaAgentä»»åŠ¡åˆ†æ"""
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´

        task_type = task_data.get("task_type", "general")
        requirements = task_data.get("input_data", {}).get("requirements", [])
        constraints = task_data.get("input_data", {}).get("constraints", [])

        # æ¨¡æ‹Ÿå¤æ‚åº¦åˆ†æ
        complexity_factors = {
            "requirements_count": len(requirements),
            "constraints_count": len(constraints),
            "technical_stack_complexity": self._analyze_technical_stack(task_data),
            "timeline_pressure": self._analyze_timeline_pressure(task_data)
        }

        complexity_score = sum(complexity_factors.values())

        if complexity_score < 15:
            complexity_level = "low"
        elif complexity_score < 30:
            complexity_level = "medium"
        else:
            complexity_level = "high"

        analysis_result = {
            "agent_id": "meta_agent_001",
            "analysis_id": str(uuid.uuid4()),
            "task_type": task_type,
            "complexity_level": complexity_level,
            "complexity_score": complexity_score,
            "factors": complexity_factors,
            "needs_decomposition": complexity_level in ["medium", "high"],
            "recommended_approach": self._determine_approach(complexity_level, task_data),
            "estimated_duration": self._estimate_duration(complexity_level),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

        self.log_execution("MetaAgent", "AnalysisComplete", analysis_result)
        return analysis_result

    def _analyze_technical_stack(self, task_data: Dict[str, Any]) -> int:
        """åˆ†ææŠ€æœ¯æ ˆå¤æ‚åº¦"""
        stack = task_data.get("input_data", {}).get("technical_stack", {})
        complexity = 0

        if stack.get("backend"):
            if "Python" in stack["backend"]:
                complexity += 2
            if "FastAPI" in stack["backend"]:
                complexity += 2
            if "LangGraph" in stack["backend"]:
                complexity += 3

        if stack.get("frontend"):
            if "JavaScript" in stack["frontend"]:
                complexity += 2
            if "React" in stack["frontend"]:
                complexity += 3
            if "Vue" in stack["frontend"]:
                complexity += 2

        if stack.get("database"):
            if "SQLite" in stack["database"]:
                complexity += 1
            if "Redis" in stack["database"]:
                complexity += 1
            if "PostgreSQL" in stack["database"]:
                complexity += 3

        if stack.get("ai_models"):
            complexity += len(stack["ai_models"]) * 2

        return complexity

    def _analyze_timeline_pressure(self, task_data: Dict[str, Any]) -> int:
        """åˆ†ææ—¶é—´çº¿å‹åŠ›"""
        deadline = task_data.get("deadline")
        if not deadline:
            return 0

        try:
            deadline_dt = datetime.fromisoformat(deadline.replace('Z', '+00:00'))
            days_remaining = (deadline_dt - datetime.now(timezone.utc)).days

            if days_remaining < 7:
                return 5
            elif days_remaining < 30:
                return 3
            else:
                return 0
        except:
            return 0

    def _determine_approach(self, complexity_level: str, task_data: Dict[str, Any]) -> str:
        """ç¡®å®šå®æ–½æ–¹æ³•"""
        if complexity_level == "high":
            return "coordinated_development"
        elif complexity_level == "medium":
            return "structured_development"
        else:
            return "iterative_development"

    def _estimate_duration(self, complexity_level: str) -> int:
        """ä¼°ç®—æŒç»­æ—¶é—´ï¼ˆå¤©ï¼‰"""
        duration_map = {
            "low": 7,
            "medium": 21,
            "high": 42
        }
        return duration_map.get(complexity_level, 14)

    async def _simulate_task_decomposition(self, task_data: Dict[str, Any], meta_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸTaskDecomposerä»»åŠ¡åˆ†è§£"""
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿåˆ†è§£æ—¶é—´

        requirements = task_data.get("input_data", {}).get("requirements", [])

        # åˆ›å»ºå­ä»»åŠ¡
        subtasks = []

        # åŸºäºéœ€æ±‚åˆ›å»ºå­ä»»åŠ¡
        requirement_mapping = {
            "è‡ªç„¶è¯­è¨€å¯¹è¯äº¤äº’": ["å¯¹è¯å¼•æ“å¼€å‘", "è‡ªç„¶è¯­è¨€å¤„ç†", "å¯¹è¯å†å²ç®¡ç†"],
            "ä»»åŠ¡ç®¡ç†åŠŸèƒ½": ["ä»»åŠ¡åˆ›å»ºæ¨¡å—", "ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª", "ä»»åŠ¡ä¼˜å…ˆçº§ç®¡ç†"],
            "æ—¥ç¨‹å®‰æ’ç®¡ç†": ["æ—¥å†é›†æˆ", "äº‹ä»¶æé†’", "æ—¶é—´è§„åˆ’"],
            "ä¿¡æ¯æŸ¥è¯¢å’Œæœç´¢": ["æœç´¢å¼•æ“", "æ•°æ®æ£€ç´¢", "ä¿¡æ¯èšåˆ"],
            "ä¸ªæ€§åŒ–æ¨è": ["æ¨èç®—æ³•", "ç”¨æˆ·åå¥½å­¦ä¹ ", "å†…å®¹è¿‡æ»¤"],
            "æ•°æ®åˆ†æå’Œå¯è§†åŒ–": ["æ•°æ®æ”¶é›†", "åˆ†æå¼•æ“", "å¯è§†åŒ–ç»„ä»¶"],
            "çŸ¥è¯†åº“ç®¡ç†": ["çŸ¥è¯†å­˜å‚¨", "æ™ºèƒ½æ£€ç´¢", "çŸ¥è¯†æ›´æ–°"],
            "å¤šæ¨¡æ€äº¤äº’æ”¯æŒ": ["å›¾åƒå¤„ç†", "è¯­éŸ³è¯†åˆ«", "å¤šæ¨¡æ€èåˆ"]
        }

        subtask_id = 1
        for requirement in requirements:
            if requirement in requirement_mapping:
                for subtask_name in requirement_mapping[requirement]:
                    subtask = {
                        "subtask_id": f"subtask_{subtask_id:03d}",
                        "name": subtask_name,
                        "parent_requirement": requirement,
                        "description": f"å®ç°{subtask_name}åŠŸèƒ½",
                        "priority": "high",
                        "estimated_days": 5,
                        "dependencies": [],
                        "required_skills": ["Python", "AI/ML", "Webå¼€å‘"]
                    }
                    subtasks.append(subtask)
                    subtask_id += 1

        # åˆ†æä¾èµ–å…³ç³»
        dependency_graph = {}
        for i, subtask in enumerate(subtasks):
            deps = []
            # æ¨¡æ‹Ÿä¸€äº›ä¾èµ–å…³ç³»
            if i > 0:
                deps.append(subtasks[i-1]["subtask_id"])
            dependency_graph[subtask["subtask_id"]] = deps

        # åˆ›å»ºå·¥ä½œæµè®¡åˆ’
        workflow_plan = {
            "parallel_groups": [
                [subtasks[0]["subtask_id"], subtasks[1]["subtask_id"]],  # é˜¶æ®µ1ï¼šåŸºç¡€æ¡†æ¶
                [subtasks[2]["subtask_id"], subtasks[3]["subtask_id"]],  # é˜¶æ®µ2ï¼šæ ¸å¿ƒåŠŸèƒ½
                [subtasks[4]["subtask_id"], subtasks[5]["subtask_id"]],  # é˜¶æ®µ3ï¼šæ™ºèƒ½åŠŸèƒ½
                [subtasks[6]["subtask_id"], subtasks[7]["subtask_id"]]   # é˜¶æ®µ4ï¼šé«˜çº§åŠŸèƒ½
            ],
            "critical_path": [subtask["subtask_id"] for subtask in subtasks],
            "estimated_total_days": max([subtask.get("estimated_days", 0) for subtask in subtasks]) * len(subtasks),
            "parallel_execution": True
        }

        decomposition_result = {
            "agent_id": "task_decomposer_001",
            "decomposition_id": str(uuid.uuid4()),
            "original_task_id": task_data["task_id"],
            "subtasks_count": len(subtasks),
            "subtasks": subtasks,
            "dependency_graph": dependency_graph,
            "workflow_plan": workflow_plan,
            "complexity_score": meta_result.get("complexity_score", 0),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

        self.log_execution("TaskDecomposer", "DecompositionComplete", {
            "subtasks_count": len(subtasks),
            "workflow_groups": len(workflow_plan["parallel_groups"])
        })
        return decomposition_result

    async def _simulate_coordination(self, decomposition_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸCoordinatorAgentåè°ƒ"""
        await asyncio.sleep(1.5)  # æ¨¡æ‹Ÿåè°ƒæ—¶é—´

        subtasks = decomposition_result.get("subtasks", [])
        workflow_plan = decomposition_result.get("workflow_plan", {})
        parallel_groups = workflow_plan.get("parallel_groups", [])

        coordination_result = {
            "agent_id": "coordinator_001",
            "coordination_id": str(uuid.uuid4()),
            "assigned_agents": [],
            "execution_plan": {},
            "resource_allocation": {},
            "timeline": {}
        }

        # ä¸ºæ¯ä¸ªå¹¶è¡Œç»„åˆ†é…èµ„æº
        total_duration = 0
        for i, group in enumerate(parallel_groups):
            group_duration = 0
            group_agents = []

            for j, subtask_id in enumerate(group):
                # æŸ¥æ‰¾å­ä»»åŠ¡
                subtask = next((s for s in subtasks if s["subtask_id"] == subtask_id), None)
                if subtask:
                    # åˆ†é…æ™ºèƒ½ä½“
                    agent_id = f"agent_{i+1}_{j+1}"
                    group_agents.append(agent_id)

                    # åˆ†é…èµ„æº
                    resource_allocation = {
                        "cpu_cores": 2,
                        "memory_gb": 4,
                        "storage_gb": 10,
                        "estimated_duration": subtask.get("estimated_days", 5)
                    }

                    coordination_result["resource_allocation"][agent_id] = resource_allocation
                    group_duration = max(group_duration, resource_allocation["estimated_duration"])

            coordination_result["execution_plan"][f"group_{i+1}"] = {
                "subtasks": group,
                "agents": group_agents,
                "duration_days": group_duration,
                "start_day": total_duration + 1
            }

            coordination_result["assigned_agents"].extend(group_agents)
            total_duration += group_duration

        coordination_result["total_duration_days"] = total_duration
        coordination_result["estimated_completion_date"] = (
            datetime.now(timezone.utc) + timedelta(days=total_duration)
        ).isoformat()

        self.log_execution("Coordinator", "CoordinationComplete", {
            "groups_count": len(parallel_groups),
            "total_duration_days": total_duration,
            "agents_assigned": len(coordination_result["assigned_agents"])
        })
        return coordination_result

    async def _simulate_task_execution(self, coordination_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ"""
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´

        execution_result = {
            "execution_id": str(uuid.uuid4()),
            "execution_status": "in_progress",
            "progress": {},
            "completed_subtasks": [],
            "failed_subtasks": [],
            "quality_metrics": {}
        }

        # æ¨¡æ‹Ÿæ¯ä¸ªç»„çš„æ‰§è¡Œè¿›åº¦
        execution_plan = coordination_result.get("execution_plan", {})

        for group_name, group_info in execution_plan.items():
            group_progress = {}
            subtasks = group_info.get("subtasks", [])

            for subtask_id in subtasks:
                # æ¨¡æ‹Ÿå­ä»»åŠ¡æ‰§è¡Œ
                await asyncio.sleep(0.2)
                progress = min(100, len(group_progress) * 20 + 20)
                group_progress[subtask_id] = progress

                if progress >= 100:
                    execution_result["completed_subtasks"].append(subtask_id)
                    quality_score = 85 + (hash(subtask_id) % 15)  # æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°
                    execution_result["quality_metrics"][subtask_id] = quality_score
                else:
                    group_progress[subtask_id] = progress

            execution_result["progress"][group_name] = {
                "overall_progress": sum(group_progress.values()) / len(group_progress),
                "subtasks_progress": group_progress,
                "status": "completed" if all(p >= 100 for p in group_progress.values()) else "in_progress"
            }

        # è®¡ç®—æ€»ä½“è¿›åº¦
        all_progress = []
        for group_info in execution_result["progress"].values():
            all_progress.append(group_info["overall_progress"])

        overall_progress = sum(all_progress) / len(all_progress) if all_progress else 0
        execution_result["overall_progress"] = overall_progress
        execution_result["execution_status"] = "completed" if overall_progress >= 100 else "in_progress"

        if overall_progress >= 100:
            execution_result["completion_time"] = datetime.now(timezone.utc).isoformat()
            execution_result["quality_score"] = sum(
                execution_result["quality_metrics"].get(subtask_id, 85)
                for subtask_id in execution_result["quality_metrics"]
            ) / len(execution_result["quality_metrics"]) if execution_result["quality_metrics"] else 85

        self.log_execution("Executor", "ExecutionComplete", {
            "overall_progress": overall_progress,
            "completed_subtasks": len(execution_result["completed_subtasks"]),
            "quality_score": execution_result.get("quality_score", 0)
        })
        return execution_result

    async def _generate_summary(self, workflow_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        summary = {
            "summary_id": str(uuid.uuid4()),
            "task_info": {
                "task_id": workflow_result["task_id"],
                "task_title": workflow_result["task_title"],
                "execution_id": workflow_result["execution_id"]
            },
            "execution_summary": {
                "status": workflow_result.get("status", "unknown"),
                "success": workflow_result.get("success", False),
                "duration_seconds": workflow_result.get("duration_seconds", 0),
                "phases_completed": len(workflow_result.get("phases", [])),
                "start_time": workflow_result.get("start_time"),
                "end_time": workflow_result.get("end_time")
            },
            "phase_results": {},
            "key_metrics": {},
            "recommendations": []
        }

        # æ”¶é›†å„é˜¶æ®µç»“æœ
        for phase in workflow_result.get("phases", []):
            phase_name = phase["phase"]
            phase_result = phase.get("result", {})

            summary["phase_results"][phase_name] = {
                "status": phase.get("status"),
                "duration": 0,  # è¿™é‡Œå¯ä»¥è®¡ç®—å„é˜¶æ®µè€—æ—¶
                "key_outputs": []
            }

            # æå–å…³é”®è¾“å‡º
            if phase_name == "meta_analysis":
                summary["phase_results"][phase_name]["key_outputs"] = [
                    f"å¤æ‚åº¦çº§åˆ«: {phase_result.get('complexity_level', 'unknown')}",
                    f"åˆ†è§£éœ€æ±‚: {phase_result.get('needs_decomposition', False)}"
                ]
            elif phase_name == "task_decomposition":
                summary["phase_results"][phase_name]["key_outputs"] = [
                    f"å­ä»»åŠ¡æ•°é‡: {phase_result.get('subtasks_count', 0)}",
                    f"å¹¶è¡Œç»„æ•°: {len(phase_result.get('workflow_plan', {}).get('parallel_groups', []))}"
                ]
            elif phase_name == "coordination":
                summary["phase_results"][phase_name]["key_outputs"] = [
                    f"åè°ƒæ™ºèƒ½ä½“: {phase_result.get('agent_id', 'unknown')}",
                    f"åˆ†é…ä»»åŠ¡ç»„: {len(phase_result.get('execution_plan', {}))}"
                ]
            elif phase_name == "execution":
                summary["phase_results"][phase_name]["key_outputs"] = [
                    f"å®Œæˆè¿›åº¦: {phase_result.get('overall_progress', 0):.1f}%",
                    f"è´¨é‡è¯„åˆ†: {phase_result.get('quality_score', 0):.1f}"
                ]

        # è®¡ç®—å…³é”®æŒ‡æ ‡
        meta_phase = next((p for p in workflow_result.get("phases", []) if p["phase"] == "meta_analysis"), None)
        if meta_phase:
            meta_result = meta_phase.get("result", {})
            summary["key_metrics"]["complexity_level"] = meta_result.get("complexity_level")
            summary["key_metrics"]["complexity_score"] = meta_result.get("complexity_score")
            summary["key_metrics"]["estimated_duration"] = meta_result.get("estimated_duration")

        decomp_phase = next((p for p in workflow_result.get("phases", []) if p["phase"] == "task_decomposition"), None)
        if decomp_phase:
            decomp_result = decomp_phase.get("result", {})
            summary["key_metrics"]["subtasks_created"] = decomp_result.get("subtasks_count", 0)
            summary["key_metrics"]["workflow_groups"] = len(decomp_result.get("workflow_plan", {}).get("parallel_groups", []))

        exec_phase = next((p for p in workflow_result.get("phases", []) if p["phase"] == "execution"), None)
        if exec_phase:
            exec_result = exec_phase.get("result", {})
            summary["key_metrics"]["overall_progress"] = exec_result.get("overall_progress", 0)
            summary["key_metrics"]["quality_score"] = exec_result.get("quality_score", 0)

        # ç”Ÿæˆå»ºè®®
        recommendations = []

        if summary["key_metrics"].get("complexity_level") == "high":
            recommendations.append("å»ºè®®åˆ†é˜¶æ®µå®æ–½ï¼Œä¼˜å…ˆå®ç°æ ¸å¿ƒåŠŸèƒ½")

        if summary["key_metrics"].get("overall_progress", 0) < 100:
            recommendations.append("ä»»åŠ¡æ‰§è¡Œæœªå®Œæˆï¼Œå»ºè®®æ£€æŸ¥ä¾èµ–å…³ç³»")

        if summary["key_metrics"].get("quality_score", 0) < 80:
            recommendations.append("å»ºè®®å¢åŠ æµ‹è¯•å’Œè´¨é‡ä¿è¯ç¯èŠ‚")

        summary["recommendations"] = recommendations

        self.log_execution("System", "SummaryGenerated", {
            "recommendations_count": len(recommendations),
            "quality_score": summary["key_metrics"].get("quality_score", 0)
        })
        return summary

    async def _save_results(self, final_result: Dict[str, Any]):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.results_dir / f"agent_system_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)

        # ä¿å­˜æ‰§è¡Œæ—¥å¿—
        log_file = self.results_dir / f"execution_log_{timestamp}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(self.execution_log, f, ensure_ascii=False, indent=2)

        # ä¿å­˜å¯è¯»æŠ¥å‘Š
        report_file = self.results_dir / f"execution_report_{timestamp}.md"
        await self._generate_readable_report(final_result, report_file)

        # ä¿å­˜ä»»åŠ¡æ•°æ®
        task_file = self.results_dir / f"task_data_{timestamp}.json"
        with open(task_file, 'w', encoding='utf-8') as f:
            json.dump(final_result.get("phase_results", {}), f, ensure_ascii=False, indent=2)

        # è¿”å›æ–‡ä»¶è·¯å¾„
        return {
            "results_file": str(results_file),
            "log_file": str(log_file),
            "report_file": str(report_file),
            "task_data_file": str(task_file),
            "timestamp": timestamp
        }

    async def _generate_readable_report(self, results: Dict[str, Any], output_file: Path):
        """ç”Ÿæˆå¯è¯»çš„æŠ¥å‘Š"""
        report = []
        report.append("# ä¸ªäººAIåŠ©æ‰‹ä»»åŠ¡æ‰§è¡ŒæŠ¥å‘Š")
        report.append(f"æ‰§è¡Œæ—¶é—´: {results.get('end_time', 'Unknown')}")
        report.append(f"æ‰§è¡ŒID: {results.get('execution_id', 'Unknown')}")
        report.append(f"ä»»åŠ¡ID: {results.get('task_id', 'Unknown')}")
        report.append(f"ä»»åŠ¡æ ‡é¢˜: {results.get('task_title', 'Unknown')}")
        report.append("")

        # æ‰§è¡Œæ¦‚è¦
        report.append("## æ‰§è¡Œæ¦‚è¦")
        report.append(f"- **çŠ¶æ€**: {'æˆåŠŸ' if results.get('success') else 'å¤±è´¥'}")
        report.append(f"- **è€—æ—¶**: {results.get('duration_seconds', 0):.2f} ç§’")
        report.append(f"- **é˜¶æ®µæ•°**: {len(results.get('phases', []))}")
        report.append("")

        # é˜¶æ®µè¯¦æƒ…
        report.append("## ğŸ”„ æ‰§è¡Œé˜¶æ®µè¯¦æƒ…")
        for i, phase in enumerate(results.get("phases", []), 1):
            report.append(f"### é˜¶æ®µ {i}: {phase['phase'].title()}")
            report.append(f"- **çŠ¶æ€**: {phase.get('status', 'Unknown')}")
            report.append(f"- **æ—¶é—´**: {phase.get('timestamp', 'Unknown')}")

            result = phase.get("result", {})
            if "complexity_level" in result:
                report.append(f"- **å¤æ‚åº¦**: {result['complexity_level']}")
            if "subtasks_count" in result:
                report.append(f"- **å­ä»»åŠ¡æ•°**: {result['subtasks_count']}")
            if "overall_progress" in result:
                report.append(f"- **å®Œæˆåº¦**: {result['overall_progress']:.1f}%")
            if "quality_score" in result:
                report.append(f"- **è´¨é‡è¯„åˆ†**: {result['quality_score']:.1f}/100")
            report.append("")

        # å…³é”®æŒ‡æ ‡
        report.append("## å…³é”®æŒ‡æ ‡")
        execution_summary = results.get("execution_summary", {})
        summary = results.get("key_metrics", {})

        for metric_name, metric_value in summary.items():
            report.append(f"- **{metric_name}**: {metric_value}")
        report.append("")

        # å»ºè®®
        report.append("## æ”¹è¿›å»ºè®®")
        phase_summary = results.get("phase_results", {})
        summary_phase = phase_summary.get("summary", {})
        recommendations = summary_phase.get("recommendations", [])

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                report.append(f"{i}. {rec}")
        else:
            report.append("- æ— é‡å¤§é—®é¢˜ï¼Œæ‰§è¡Œè‰¯å¥½")
        report.append("")

        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

async def main():
    """ä¸»å‡½æ•°"""
    demo = AgentSystemDemo()

    # è¿è¡Œæ¼”ç¤º
    task_file = "examples/personal_ai_assistant_task.json"
    results = await demo.run_demo(task_file)

    # è¾“å‡ºç»“æœä¿¡æ¯
    print("\n" + "="*80)
    print("ä¸ªäººAIåŠ©æ‰‹ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼")
    print("="*80)
    print(f"ä»»åŠ¡æ ‡é¢˜: {results['task_title']}")
    print(f"æ‰§è¡ŒçŠ¶æ€: {'æˆåŠŸ' if results['success'] else 'å¤±è´¥'}")
    print(f"æ‰§è¡Œè€—æ—¶: {results['duration_seconds']:.2f} ç§’")
    print(f"å®Œæˆé˜¶æ®µ: {len(results['phases'])}")
    print(f"è´¨é‡è¯„åˆ†: {results.get('key_metrics', {}).get('quality_score', 0):.1f}/100")
    print("\nç»“æœæ–‡ä»¶ä¿å­˜ä½ç½®:")

    file_paths = results.get("file_paths", {})
    if file_paths:
        for file_type, file_path in file_paths.items():
            print(f"  {file_type}: {file_path}")

    print("\næŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š:")
    print(f"  {file_paths.get('report_file', '')}")

    return results

if __name__ == "__main__":
    asyncio.run(main())